{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex 67ca063..6794b98 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -681,6 +681,29 @@ class GaussianMixture(BaseMixture):\n     def _compute_lower_bound(self, _, log_prob_norm):\n         return log_prob_norm\n \n+    def fit(self, X, y=None):\n+        \"\"\"Estimate model parameters with the EM algorithm.\n+\n+        A initialization step is performed before entering the\n+        expectation-maximization (EM) algorithm. If you want to avoid\n+        this step, set the keyword argument init_params to the empty\n+        string '' when creating the GaussianMixture object. Likewise, if you would\n+        like just to do an initialization, set n_iter=0.\n+\n+        Parameters\n+        ----------\n+        X : array_like, shape (n, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        self\n+        \"\"\"\n+        self._fit(X, y)\n+        self.labels_ = self.predict(X)\n+        return self\n+\n     def _check_is_fitted(self):\n         check_is_fitted(self, ['weights_', 'means_', 'precisions_cholesky_'])\n \n@@ -720,6 +743,23 @@ class GaussianMixture(BaseMixture):\n         mean_params = n_features * self.n_components\n         return int(cov_params + mean_params + self.n_components - 1)\n \n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model to X and then predict the labels for the data samples in X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component memberships for each sample.\n+        \"\"\"\n+        self.fit(X)\n+        return self.predict(X)\n+\n     def bic(self, X):\n         \"\"\"Bayesian information criterion for the current model on the input X.\n \n@@ -747,4 +787,4 @@ class GaussianMixture(BaseMixture):\n         aic : float\n             The lower the better.\n         \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n", "occurrence": 1, "normalized_patch": "--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n+    def fit(self, X, y=None):\n+        self._fit(X, y)\n+        self.labels_ = self.predict(X)\n+        return self\n+    def fit_predict(self, X, y=None):\n+        self.fit(X)\n+        return self.predict(X)", "normalized_occurrence": 2}
