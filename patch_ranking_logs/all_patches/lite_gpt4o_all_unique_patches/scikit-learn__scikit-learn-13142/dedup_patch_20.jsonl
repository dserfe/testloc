{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13142", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a853e4a..97f8a17 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -188,92 +188,8 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         -------\n         self\n         \"\"\"\n-        self.fit_predict(X, y)\n-        return self\n-\n-    def fit_predict(self, X, y=None):\n-        \"\"\"Estimate model parameters using X and predict the labels for X.\n-\n-        The method fits the model n_init times and sets the parameters with\n-        which the model has the largest likelihood or lower bound. Within each\n-        trial, the method iterates between E-step and M-step for `max_iter`\n-        times until the change of likelihood or lower bound is less than\n-        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n-        predicts the most probable label for the input data points.\n-\n-        .. versionadded:: 0.20\n-\n-        Parameters\n-        ----------\n-        X : array-like, shape (n_samples, n_features)\n-            List of n_features-dimensional data points. Each row\n-            corresponds to a single data point.\n-\n-        Returns\n-        -------\n-        labels : array, shape (n_samples,)\n-            Component labels.\n-        \"\"\"\n-        X = _check_X(X, self.n_components, ensure_min_samples=2)\n-        self._check_initial_parameters(X)\n-\n-        # if we enable warm_start, we will have a unique initialisation\n-        do_init = not(self.warm_start and hasattr(self, 'converged_'))\n-        n_init = self.n_init if do_init else 1\n-\n-        max_lower_bound = -np.infty\n-        self.converged_ = False\n-\n-        random_state = check_random_state(self.random_state)\n-\n-        n_samples, _ = X.shape\n-        for init in range(n_init):\n-            self._print_verbose_msg_init_beg(init)\n-\n-            if do_init:\n-                self._initialize_parameters(X, random_state)\n-\n-            lower_bound = (-np.infty if do_init else self.lower_bound_)\n-\n-            for n_iter in range(1, self.max_iter + 1):\n-                prev_lower_bound = lower_bound\n-\n-                log_prob_norm, log_resp = self._e_step(X)\n-                self._m_step(X, log_resp)\n-                lower_bound = self._compute_lower_bound(\n-                    log_resp, log_prob_norm)\n-\n-                change = lower_bound - prev_lower_bound\n-                self._print_verbose_msg_iter_end(n_iter, change)\n-\n-                if abs(change) < self.tol:\n-                    self.converged_ = True\n-                    break\n-\n-            self._print_verbose_msg_init_end(lower_bound)\n-\n-            if lower_bound > max_lower_bound:\n-                max_lower_bound = lower_bound\n-                best_params = self._get_parameters()\n-                best_n_iter = n_iter\n-\n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n-        if not self.converged_:\n-            warnings.warn('Initialization %d did not converge. '\n-                          'Try different init parameters, '\n-                          'or increase max_iter, tol '\n-                          'or check for degenerate data.'\n-                          % (init + 1), ConvergenceWarning)\n-\n-        self._set_parameters(best_params)\n-        self.n_iter_ = best_n_iter\n-        self.lower_bound_ = max_lower_bound\n-\n-        return log_resp.argmax(axis=1)\n+        self.fit(X, y)\n+        return self.predict(X)\n \n     def _e_step(self, X):\n         \"\"\"E step.\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 9518b16..6cfbb81 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -62,44 +62,8 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n     def _initialize(self, X, resp):\n         pass\n     def fit(self, X, y=None):\n-        self.fit_predict(X, y)\n-        return self\n-    def fit_predict(self, X, y=None):\n-        X = _check_X(X, self.n_components, ensure_min_samples=2)\n-        self._check_initial_parameters(X)\n-        do_init = not (self.warm_start and hasattr(self, 'converged_'))\n-        n_init = self.n_init if do_init else 1\n-        max_lower_bound = -np.infty\n-        self.converged_ = False\n-        random_state = check_random_state(self.random_state)\n-        n_samples, _ = X.shape\n-        for init in range(n_init):\n-            self._print_verbose_msg_init_beg(init)\n-            if do_init:\n-                self._initialize_parameters(X, random_state)\n-            lower_bound = -np.infty if do_init else self.lower_bound_\n-            for n_iter in range(1, self.max_iter + 1):\n-                prev_lower_bound = lower_bound\n-                log_prob_norm, log_resp = self._e_step(X)\n-                self._m_step(X, log_resp)\n-                lower_bound = self._compute_lower_bound(log_resp, log_prob_norm)\n-                change = lower_bound - prev_lower_bound\n-                self._print_verbose_msg_iter_end(n_iter, change)\n-                if abs(change) < self.tol:\n-                    self.converged_ = True\n-                    break\n-            self._print_verbose_msg_init_end(lower_bound)\n-            if lower_bound > max_lower_bound:\n-                max_lower_bound = lower_bound\n-                best_params = self._get_parameters()\n-                best_n_iter = n_iter\n-        _, log_resp = self._e_step(X)\n-        if not self.converged_:\n-            warnings.warn('Initialization %d did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.' % (init + 1), ConvergenceWarning)\n-        self._set_parameters(best_params)\n-        self.n_iter_ = best_n_iter\n-        self.lower_bound_ = max_lower_bound\n-        return log_resp.argmax(axis=1)\n+        self.fit(X, y)\n+        return self.predict(X)\n     def _e_step(self, X):\n         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n         return (np.mean(log_prob_norm), log_resp)\n", "normalized_occurrence": 1}
