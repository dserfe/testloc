{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-12682", "model_patch": "diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 8cfbe5f..4dc5708 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -926,6 +926,95 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n     Read more in the :ref:`User Guide <SparseCoder>`.\n \n+    Parameters\n+    ----------\n+    dictionary : array, [n_components, n_features]\n+        The dictionary atoms used for sparse coding. Lines are assumed to be\n+        normalized to unit norm.\n+\n+    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n+    'threshold'}\n+        Algorithm used to transform the data:\n+        lars: uses the least angle regression method (linear_model.lars_path)\n+        lasso_lars: uses Lars to compute the Lasso solution\n+        lasso_cd: uses the coordinate descent method to compute the\n+        Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n+        the estimated components are sparse.\n+        omp: uses orthogonal matching pursuit to estimate the sparse solution\n+        threshold: squashes to zero all coefficients less than alpha from\n+        the projection ``dictionary * X'``\n+\n+    transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default\n+        Number of nonzero coefficients to target in each column of the\n+        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n+        and is overridden by `alpha` in the `omp` case.\n+\n+    transform_alpha : float, 1. by default\n+        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n+        penalty applied to the L1 norm.\n+        If `algorithm='threshold'`, `alpha` is the absolute value of the\n+        threshold below which coefficients will be squashed to zero.\n+        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n+        the reconstruction error targeted. In this case, it overrides\n+        `n_nonzero_coefs`.\n+\n+    split_sign : bool, False by default\n+        Whether to split the sparse feature vector into the concatenation of\n+        its negative part and its positive part. This can improve the\n+        performance of downstream classifiers.\n+\n+    n_jobs : int or None, optional (default=None)\n+        Number of parallel jobs to run.\n+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n+        for more details.\n+\n+    positive_code : bool\n+        Whether to enforce positivity when finding the code.\n+\n+        .. versionadded:: 0.20\n+\n+    max_iter : int, 1000 by default\n+        Maximum number of iterations to perform if `algorithm='lasso_cd'`.\n+\n+    Attributes\n+    ----------\n+    components_ : array, [n_components, n_features]\n+        The unchanged dictionary atoms\n+\n+    See also\n+    --------\n+    DictionaryLearning\n+    MiniBatchDictionaryLearning\n+    SparsePCA\n+    MiniBatchSparsePCA\n+    sparse_encode\n+    \"\"\"\n+    _required_parameters = [\"dictionary\"]\n+\n+    def __init__(self, dictionary, transform_algorithm='omp',\n+                 transform_n_nonzero_coefs=None, transform_alpha=None,\n+                 split_sign=False, n_jobs=None, positive_code=False,\n+                 max_iter=1000):\n+        self._set_sparse_coding_params(dictionary.shape[0],\n+                                       transform_algorithm,\n+                                       transform_n_nonzero_coefs,\n+                                       transform_alpha, split_sign, n_jobs,\n+                                       positive_code)\n+        self.components_ = dictionary\n+        self.max_iter = max_iter\n+    \"\"\"Sparse coding\n+\n+    Finds a sparse representation of data against a fixed, precomputed\n+    dictionary.\n+\n+    Each row of the result is the solution to a sparse coding problem.\n+    The goal is to find a sparse array `code` such that::\n+\n+        X ~= code * dictionary\n+\n+    Read more in the :ref:`User Guide <SparseCoder>`.\n+\n     Parameters\n     ----------\n     dictionary : array, [n_components, n_features]\n@@ -998,6 +1087,7 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n                                        transform_alpha, split_sign, n_jobs,\n                                        positive_code)\n         self.components_ = dictionary\n+        self.max_iter = max_iter\n \n     def fit(self, X, y=None):\n         \"\"\"Do nothing and return the estimator unchanged\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 8a206a7..808b28d 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -283,10 +283,17 @@ class SparseCodingMixin(TransformerMixin):\n             code = split_code\n         return code\n class SparseCoder(BaseEstimator, SparseCodingMixin):\n+    _required_parameters = ['dictionary']\n+    def __init__(self, dictionary, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=None, positive_code=False, max_iter=1000):\n+        self._set_sparse_coding_params(dictionary.shape[0], transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs, positive_code)\n+        self.components_ = dictionary\n+        self.max_iter = max_iter\n+    \"Sparse coding\\n\\n    Finds a sparse representation of data against a fixed, precomputed\\n    dictionary.\\n\\n    Each row of the result is the solution to a sparse coding problem.\\n    The goal is to find a sparse array `code` such that::\\n\\n        X ~= code * dictionary\\n\\n    Read more in the :ref:`User Guide <SparseCoder>`.\\n\\n    Parameters\\n    ----------\\n    dictionary : array, [n_components, n_features]\\n        The dictionary atoms used for sparse coding. Lines are assumed to be\\n        normalized to unit norm.\\n\\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'}\\n        Algorithm used to transform the data:\\n        lars: uses the least angle regression method (linear_model.lars_path)\\n        lasso_lars: uses Lars to compute the Lasso solution\\n        lasso_cd: uses the coordinate descent method to compute the\\n        Lasso solution (linear_model.Lasso). lasso_lars will be faster if\\n        the estimated components are sparse.\\n        omp: uses orthogonal matching pursuit to estimate the sparse solution\\n        threshold: squashes to zero all coefficients less than alpha from\\n        the projection ``dictionary * X'``\\n\\n    transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default\\n        Number of nonzero coefficients to target in each column of the\\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\\n        and is overridden by `alpha` in the `omp` case.\\n\\n    transform_alpha : float, 1. by default\\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\\n        penalty applied to the L1 norm.\\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\\n        threshold below which coefficients will be squashed to zero.\\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\\n        the reconstruction error targeted. In this case, it overrides\\n        `n_nonzero_coefs`.\\n\\n    split_sign : bool, False by default\\n        Whether to split the sparse feature vector into the concatenation of\\n        its negative part and its positive part. This can improve the\\n        performance of downstream classifiers.\\n\\n    n_jobs : int or None, optional (default=None)\\n        Number of parallel jobs to run.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    positive_code : bool\\n        Whether to enforce positivity when finding the code.\\n\\n        .. versionadded:: 0.20\\n\\n    Attributes\\n    ----------\\n    components_ : array, [n_components, n_features]\\n        The unchanged dictionary atoms\\n\\n    See also\\n    --------\\n    DictionaryLearning\\n    MiniBatchDictionaryLearning\\n    SparsePCA\\n    MiniBatchSparsePCA\\n    sparse_encode\\n    \"\n     _required_parameters = ['dictionary']\n     def __init__(self, dictionary, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=None, positive_code=False):\n         self._set_sparse_coding_params(dictionary.shape[0], transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs, positive_code)\n         self.components_ = dictionary\n+        self.max_iter = max_iter\n     def fit(self, X, y=None):\n         return self\n class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n", "normalized_occurrence": 1}
