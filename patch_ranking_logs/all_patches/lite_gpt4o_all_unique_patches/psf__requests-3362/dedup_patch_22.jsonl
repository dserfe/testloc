{"model_name_or_path": "agentless", "instance_id": "psf__requests-3362", "model_patch": "diff --git a/requests/models.py b/requests/models.py\nindex 58593f5..a81b71b 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -666,6 +666,8 @@ class Response(object):\n             if hasattr(self.raw, 'stream'):\n                 try:\n                     for chunk in self.raw.stream(chunk_size, decode_content=True):\n+                        if decode_unicode:\n+                            chunk = chunk.decode(self.encoding or 'utf-8', errors='replace')\n                         yield chunk\n                 except ProtocolError as e:\n                     raise ChunkedEncodingError(e)\n@@ -695,7 +697,7 @@ class Response(object):\n         chunks = reused_chunks if self._content_consumed else stream_chunks\n \n         if decode_unicode:\n-            chunks = stream_decode_response_unicode(chunks, self)\n+            return stream_decode_response_unicode(chunks, self)\n \n         return chunks\n \n", "occurrence": 1, "normalized_patch": "diff --git a/requests/models.py b/requests/models.py\nindex d66e2b7..895345c 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -338,6 +338,8 @@ class Response(object):\n             if hasattr(self.raw, 'stream'):\n                 try:\n                     for chunk in self.raw.stream(chunk_size, decode_content=True):\n+                        if decode_unicode:\n+                            chunk = chunk.decode(self.encoding or 'utf-8', errors='replace')\n                         yield chunk\n                 except ProtocolError as e:\n                     raise ChunkedEncodingError(e)\n@@ -360,7 +362,7 @@ class Response(object):\n         stream_chunks = generate()\n         chunks = reused_chunks if self._content_consumed else stream_chunks\n         if decode_unicode:\n-            chunks = stream_decode_response_unicode(chunks, self)\n+            return stream_decode_response_unicode(chunks, self)\n         return chunks\n     def iter_lines(self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=None, delimiter=None):\n         pending = None\n", "normalized_occurrence": 1}
