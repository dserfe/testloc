{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-4094", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 1e17b93..f0457b0 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3457,10 +3457,13 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 if dim not in val.dims:\n                     assign_coords[dim] = None\n \n-            expand_dims = set(stacking_dims).difference(set(val.dims))\n-            expand_dims.add(variable_dim)\n-            # must be list for .expand_dims\n-            expand_dims = list(expand_dims)\n+            if stacking_dims:\n+                expand_dims = set(stacking_dims).difference(set(val.dims))\n+                expand_dims.add(variable_dim)\n+                # must be list for .expand_dims\n+                expand_dims = list(expand_dims)\n+            else:\n+                expand_dims = [variable_dim]\n \n             return (\n                 val.assign_coords(**assign_coords)\n@@ -3503,7 +3506,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         new_dim_names = index.names\n         new_dim_sizes = [lev.size for lev in index.levels]\n \n-        variables: Dict[Hashable, Variable] = {}\n+        variables: Dict[Hashable, Variable] = {name: var for name, var in self.variables.items() if name != dim}\n         indexes = {k: v for k, v in self.indexes.items() if k != dim}\n \n         for name, var in obj.variables.items():\n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 21ef2a4..3564fd6 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1150,9 +1150,12 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             for dim in stacking_dims:\n                 if dim not in val.dims:\n                     assign_coords[dim] = None\n-            expand_dims = set(stacking_dims).difference(set(val.dims))\n-            expand_dims.add(variable_dim)\n-            expand_dims = list(expand_dims)\n+            if stacking_dims:\n+                expand_dims = set(stacking_dims).difference(set(val.dims))\n+                expand_dims.add(variable_dim)\n+                expand_dims = list(expand_dims)\n+            else:\n+                expand_dims = [variable_dim]\n             return val.assign_coords(**assign_coords).expand_dims(expand_dims).stack({new_dim: (variable_dim,) + stacking_dims})\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n         data_array = xr.concat(stackable_vars, dim=new_dim)\n@@ -1173,7 +1176,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             obj = self._reindex({dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse)\n         new_dim_names = index.names\n         new_dim_sizes = [lev.size for lev in index.levels]\n-        variables: Dict[Hashable, Variable] = {}\n+        variables: Dict[Hashable, Variable] = {name: var for name, var in self.variables.items() if name != dim}\n         indexes = {k: v for k, v in self.indexes.items() if k != dim}\n         for name, var in obj.variables.items():\n             if name != dim:\n", "normalized_occurrence": 1}
