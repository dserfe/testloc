{"model_name_or_path": "agentless_claude", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 721709f..2bc5722 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -71,9 +71,12 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     provides basic common methods for mixture models.\n     \"\"\"\n \n-    def __init__(self, n_components, tol, reg_covar,\n-                 max_iter, n_init, init_params, random_state, warm_start,\n-                 verbose, verbose_interval):\n+    def __init__(self, n_components=1, n_clusters=None, tol=1e-3, reg_covar=1e-6,\n+                 max_iter=100, n_init=1, init_params='kmeans', random_state=None, \n+                 warm_start=False, verbose=0, verbose_interval=10):\n+        # For backward compatibility, n_clusters is an alias for n_components\n+        if n_clusters is not None:\n+            n_components = n_clusters\n         self.n_components = n_components\n         self.tol = tol\n         self.reg_covar = reg_covar\n@@ -239,6 +242,9 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n \n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        \n+        # Store labels for training data\n+        self.labels_ = self.predict(X)\n \n         return self\n \n@@ -322,6 +328,25 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n         \"\"\"\n         return self.score_samples(X).mean()\n \n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model and predict labels for X.\n+\n+        This is more efficient than calling fit followed by predict.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+        y : ignored\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        return self.fit(X).labels_\n+\n     def predict(self, X):\n         \"\"\"Predict the labels for the data samples in X using trained model.\n \n@@ -500,4 +525,4 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n             print(\"Initialization converged: %s\" % self.converged_)\n         elif self.verbose >= 2:\n             print(\"Initialization converged: %s\\t time lapse %.5fs\\t ll %.5f\" %\n-                  (self.converged_, time() - self._init_prev_time, ll))\n+                  (self.converged_, time() - self._init_prev_time, ll))\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 71fc3c3..7ce3119 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -22,7 +22,9 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n         raise ValueError('Expected the input data X have %d features, but got %d features' % (n_features, X.shape[1]))\n     return X\n class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n-    def __init__(self, n_components, tol, reg_covar, max_iter, n_init, init_params, random_state, warm_start, verbose, verbose_interval):\n+    def __init__(self, n_components=1, n_clusters=None, tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', random_state=None, warm_start=False, verbose=0, verbose_interval=10):\n+        if n_clusters is not None:\n+            n_components = n_clusters\n         self.n_components = n_components\n         self.tol = tol\n         self.reg_covar = reg_covar\n@@ -96,6 +98,7 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n             warnings.warn('Initialization %d did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.' % (init + 1), ConvergenceWarning)\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n         return self\n     def _e_step(self, X):\n         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n@@ -118,6 +121,8 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n         return logsumexp(self._estimate_weighted_log_prob(X), axis=1)\n     def score(self, X, y=None):\n         return self.score_samples(X).mean()\n+    def fit_predict(self, X, y=None):\n+        return self.fit(X).labels_\n     def predict(self, X):\n         self._check_is_fitted()\n         X = _check_X(X, None, self.means_.shape[1])\n", "normalized_occurrence": 1}
