{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-12682", "model_patch": "diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 8cfbe5f..3ebf585 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -313,7 +313,8 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',\n                               max_iter=max_iter,\n                               check_input=False,\n                               verbose=verbose,\n-                              positive=positive)\n+                              positive=positive,\n+                              max_iter=self.max_iter)\n         return code\n \n     # Enter parallel code block\n@@ -933,7 +934,8 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n         normalized to unit norm.\n \n     transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n-    'threshold'}\n+    'threshold'},\n+    max_iter=1000\n         Algorithm used to transform the data:\n         lars: uses the least angle regression method (linear_model.lars_path)\n         lasso_lars: uses Lars to compute the Lasso solution\n@@ -991,7 +993,8 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n     def __init__(self, dictionary, transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 split_sign=False, n_jobs=None, positive_code=False):\n+                 split_sign=False, n_jobs=None, positive_code=False, max_iter=1000):\n+        self.max_iter = max_iter\n         self._set_sparse_coding_params(dictionary.shape[0],\n                                        transform_algorithm,\n                                        transform_n_nonzero_coefs,\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 8a206a7..b3afa6f 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -83,7 +83,7 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars', n_\n         if regularization is None:\n             regularization = 1.0\n     if effective_n_jobs(n_jobs) == 1 or algorithm == 'threshold':\n-        code = _sparse_encode(X, dictionary, gram, cov=cov, algorithm=algorithm, regularization=regularization, copy_cov=copy_cov, init=init, max_iter=max_iter, check_input=False, verbose=verbose, positive=positive)\n+        code = _sparse_encode(X, dictionary, gram, cov=cov, algorithm=algorithm, regularization=regularization, copy_cov=copy_cov, init=init, max_iter=max_iter, check_input=False, verbose=verbose, positive=positive, max_iter=self.max_iter)\n         return code\n     code = np.empty((n_samples, n_components))\n     slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\n@@ -284,7 +284,8 @@ class SparseCodingMixin(TransformerMixin):\n         return code\n class SparseCoder(BaseEstimator, SparseCodingMixin):\n     _required_parameters = ['dictionary']\n-    def __init__(self, dictionary, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=None, positive_code=False):\n+    def __init__(self, dictionary, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=None, positive_code=False, max_iter=1000):\n+        self.max_iter = max_iter\n         self._set_sparse_coding_params(dictionary.shape[0], transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs, positive_code)\n         self.components_ = dictionary\n     def fit(self, X, y=None):\n", "normalized_occurrence": 1}
