{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-4094", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 1e17b93..985cc5f 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3458,7 +3458,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                     assign_coords[dim] = None\n \n             expand_dims = set(stacking_dims).difference(set(val.dims))\n-            expand_dims.add(variable_dim)\n+            if expand_dims:\n+                expand_dims.add(variable_dim)\n             # must be list for .expand_dims\n             expand_dims = list(expand_dims)\n \n@@ -3488,7 +3489,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         return data_array\n \n     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> \"Dataset\":\n+        # Account for single-dim variable case\n         index = self.get_index(dim)\n+        if isinstance(index, pd.Index):\n+            index = pd.MultiIndex.from_arrays([index], names=[dim])\n         index = remove_unused_levels_categories(index)\n         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n \n@@ -3518,7 +3522,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             variables[name] = IndexVariable(name, lev)\n             indexes[name] = lev\n \n+        # Avoid conflicts with single-dim variables during unstacking\n         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)\n+        if len(new_dim_names) == 1:\n+            coord_names.add(new_dim_names[0])\n \n         return self._replace_with_new_dims(\n             variables, coord_names=coord_names, indexes=indexes\n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 21ef2a4..7ae604f 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1151,7 +1151,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 if dim not in val.dims:\n                     assign_coords[dim] = None\n             expand_dims = set(stacking_dims).difference(set(val.dims))\n-            expand_dims.add(variable_dim)\n+            if expand_dims:\n+                expand_dims.add(variable_dim)\n             expand_dims = list(expand_dims)\n             return val.assign_coords(**assign_coords).expand_dims(expand_dims).stack({new_dim: (variable_dim,) + stacking_dims})\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n@@ -1165,6 +1166,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         return data_array\n     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> 'Dataset':\n         index = self.get_index(dim)\n+        if isinstance(index, pd.Index):\n+            index = pd.MultiIndex.from_arrays([index], names=[dim])\n         index = remove_unused_levels_categories(index)\n         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n         if index.equals(full_idx):\n@@ -1186,6 +1189,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             variables[name] = IndexVariable(name, lev)\n             indexes[name] = lev\n         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)\n+        if len(new_dim_names) == 1:\n+            coord_names.add(new_dim_names[0])\n         return self._replace_with_new_dims(variables, coord_names=coord_names, indexes=indexes)\n     def unstack(self, dim: Union[Hashable, Iterable[Hashable]]=None, fill_value: Any=dtypes.NA, sparse: bool=False) -> 'Dataset':\n         if dim is None:\n", "normalized_occurrence": 1}
