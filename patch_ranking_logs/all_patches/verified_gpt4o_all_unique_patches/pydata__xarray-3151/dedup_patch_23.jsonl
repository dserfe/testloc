{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-3151", "model_patch": "diff --git a/xarray/core/combine.py b/xarray/core/combine.py\nindex fe3f23b..a182106 100644\n--- a/xarray/core/combine.py\n+++ b/xarray/core/combine.py\n@@ -66,42 +66,33 @@ def _infer_concat_order_from_coords(datasets):\n                 raise ValueError(\"Every dimension needs a coordinate for \"\n                                  \"inferring concatenation order\")\n \n-            # If dimension coordinate values are same on every dataset then\n-            # should be leaving this dimension alone (it's just a \"bystander\")\n-            if not all(index.equals(indexes[0]) for index in indexes[1:]):\n-\n-                # Infer order datasets should be arranged in along this dim\n-                concat_dims.append(dim)\n-\n-                if all(index.is_monotonic_increasing for index in indexes):\n-                    ascending = True\n-                elif all(index.is_monotonic_decreasing for index in indexes):\n-                    ascending = False\n-                else:\n-                    raise ValueError(\"Coordinate variable {} is neither \"\n-                                     \"monotonically increasing nor \"\n-                                     \"monotonically decreasing on all datasets\"\n-                                     .format(dim))\n-\n-                # Assume that any two datasets whose coord along dim starts\n-                # with the same value have the same coord values throughout.\n-                if any(index.size == 0 for index in indexes):\n-                    raise ValueError('Cannot handle size zero dimensions')\n-                first_items = pd.Index([index.take([0])\n-                                        for index in indexes])\n-\n-                # Sort datasets along dim\n-                # We want rank but with identical elements given identical\n-                # position indices - they should be concatenated along another\n-                # dimension, not along this one\n-                series = first_items.to_series()\n-                rank = series.rank(method='dense', ascending=ascending)\n-                order = rank.astype(int).values - 1\n-\n-                # Append positions along extra dimension to structure which\n-                # encodes the multi-dimensional concatentation order\n-                tile_ids = [tile_id + (position,) for tile_id, position\n-                            in zip(tile_ids, order)]\n+            if any(index.size == 0 for index in indexes):\n+                raise ValueError('Cannot handle size zero dimensions')\n+\n+            if all(index.equals(indexes[0]) for index in indexes[1:]):\n+                continue\n+\n+            concat_dims.append(dim)\n+\n+            if all(index.is_monotonic_increasing for index in indexes):\n+                ascending = True\n+            elif all(index.is_monotonic_decreasing for index in indexes):\n+                ascending = False\n+            else:\n+                raise ValueError(\"Coordinate variable {} is neither \"\n+                                 \"monotonically increasing nor \"\n+                                 \"monotonically decreasing on all datasets\"\n+                                 .format(dim))\n+\n+            first_items = pd.Index([index.take([0])\n+                                    for index in indexes])\n+\n+            series = first_items.to_series()\n+            rank = series.rank(method='dense', ascending=ascending)\n+            order = rank.astype(int).values - 1\n+\n+            tile_ids = [tile_id + (position,) for tile_id, position\n+                        in zip(tile_ids, order)]\n \n     if len(datasets) > 1 and not concat_dims:\n         raise ValueError(\"Could not find any dimension coordinates to use to \"\n@@ -500,15 +491,7 @@ def combine_by_coords(datasets, compat='no_conflicts', data_vars='all',\n                                    data_vars=data_vars, coords=coords,\n                                    fill_value=fill_value)\n \n-        # Check the overall coordinates are monotonically increasing\n-        for dim in concatenated.dims:\n-            if dim in concatenated:\n-                indexes = concatenated.indexes.get(dim)\n-                if not (indexes.is_monotonic_increasing\n-                        or indexes.is_monotonic_decreasing):\n-                    raise ValueError(\"Resulting object does not have monotonic\"\n-                                     \" global indexes along dimension {}\"\n-                                     .format(dim))\n+        # Skip the monotonicity check for concatenated coordinates\n         concatenated_grouped_by_data_vars.append(concatenated)\n \n     return merge(concatenated_grouped_by_data_vars, compat=compat,\n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/combine.py b/xarray/core/combine.py\nindex b8f5640..163b68c 100644\n--- a/xarray/core/combine.py\n+++ b/xarray/core/combine.py\n@@ -27,21 +27,22 @@ def _infer_concat_order_from_coords(datasets):\n             indexes = [ds.indexes.get(dim) for ds in datasets]\n             if any((index is None for index in indexes)):\n                 raise ValueError('Every dimension needs a coordinate for inferring concatenation order')\n-            if not all((index.equals(indexes[0]) for index in indexes[1:])):\n-                concat_dims.append(dim)\n-                if all((index.is_monotonic_increasing for index in indexes)):\n-                    ascending = True\n-                elif all((index.is_monotonic_decreasing for index in indexes)):\n-                    ascending = False\n-                else:\n-                    raise ValueError('Coordinate variable {} is neither monotonically increasing nor monotonically decreasing on all datasets'.format(dim))\n-                if any((index.size == 0 for index in indexes)):\n-                    raise ValueError('Cannot handle size zero dimensions')\n-                first_items = pd.Index([index.take([0]) for index in indexes])\n-                series = first_items.to_series()\n-                rank = series.rank(method='dense', ascending=ascending)\n-                order = rank.astype(int).values - 1\n-                tile_ids = [tile_id + (position,) for tile_id, position in zip(tile_ids, order)]\n+            if any((index.size == 0 for index in indexes)):\n+                raise ValueError('Cannot handle size zero dimensions')\n+            if all((index.equals(indexes[0]) for index in indexes[1:])):\n+                continue\n+            concat_dims.append(dim)\n+            if all((index.is_monotonic_increasing for index in indexes)):\n+                ascending = True\n+            elif all((index.is_monotonic_decreasing for index in indexes)):\n+                ascending = False\n+            else:\n+                raise ValueError('Coordinate variable {} is neither monotonically increasing nor monotonically decreasing on all datasets'.format(dim))\n+            first_items = pd.Index([index.take([0]) for index in indexes])\n+            series = first_items.to_series()\n+            rank = series.rank(method='dense', ascending=ascending)\n+            order = rank.astype(int).values - 1\n+            tile_ids = [tile_id + (position,) for tile_id, position in zip(tile_ids, order)]\n     if len(datasets) > 1 and (not concat_dims):\n         raise ValueError('Could not find any dimension coordinates to use to order the datasets for concatenation')\n     combined_ids = OrderedDict(zip(tile_ids, datasets))\n@@ -115,11 +116,6 @@ def combine_by_coords(datasets, compat='no_conflicts', data_vars='all', coords='\n         combined_ids, concat_dims = _infer_concat_order_from_coords(list(datasets_with_same_vars))\n         _check_shape_tile_ids(combined_ids)\n         concatenated = _combine_nd(combined_ids, concat_dims=concat_dims, data_vars=data_vars, coords=coords, fill_value=fill_value)\n-        for dim in concatenated.dims:\n-            if dim in concatenated:\n-                indexes = concatenated.indexes.get(dim)\n-                if not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing):\n-                    raise ValueError('Resulting object does not have monotonic global indexes along dimension {}'.format(dim))\n         concatenated_grouped_by_data_vars.append(concatenated)\n     return merge(concatenated_grouped_by_data_vars, compat=compat, fill_value=fill_value)\n _CONCAT_DIM_DEFAULT = '__infer_concat_dim__'\n", "normalized_occurrence": 1}
