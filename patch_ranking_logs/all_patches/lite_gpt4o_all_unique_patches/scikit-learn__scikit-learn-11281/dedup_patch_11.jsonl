{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex 67ca063..437c7eb 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -432,8 +432,10 @@ def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n     return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n \n \n-class GaussianMixture(BaseMixture):\n+class GaussianMixture(BaseMixture, ClusterMixin):\n     \"\"\"Gaussian Mixture.\n+        This class allows to estimate the parameters of a Gaussian mixture\n+        distribution. It is also clusterer-compatible.\n \n     Representation of a Gaussian mixture model probability distribution.\n     This class allows to estimate the parameters of a Gaussian mixture\n@@ -592,6 +594,7 @@ class GaussianMixture(BaseMixture):\n             verbose=verbose, verbose_interval=verbose_interval)\n \n         self.covariance_type = covariance_type\n+        self.labels_ = None\n         self.weights_init = weights_init\n         self.means_init = means_init\n         self.precisions_init = precisions_init\n@@ -720,6 +723,22 @@ class GaussianMixture(BaseMixture):\n         mean_params = n_features * self.n_components\n         return int(cov_params + mean_params + self.n_components - 1)\n \n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model to the data and predict labels for the data.\n+        \n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component memberships.\n+        \"\"\"\n+        self.fit(X, y)\n+        self.labels_ = self.predict(X)\n+        return self.labels_\n+\n     def bic(self, X):\n         \"\"\"Bayesian information criterion for the current model on the input X.\n \n@@ -747,4 +766,4 @@ class GaussianMixture(BaseMixture):\n         aic : float\n             The lower the better.\n         \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex ff762d7..e10a87b 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -115,10 +115,11 @@ def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n         precisions = precisions_chol ** 2\n         log_prob = np.sum(means ** 2, 1) * precisions - 2 * np.dot(X, means.T * precisions) + np.outer(row_norms(X, squared=True), precisions)\n     return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n-class GaussianMixture(BaseMixture):\n+class GaussianMixture(BaseMixture, ClusterMixin):\n     def __init__(self, n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10):\n         super(GaussianMixture, self).__init__(n_components=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval)\n         self.covariance_type = covariance_type\n+        self.labels_ = None\n         self.weights_init = weights_init\n         self.means_init = means_init\n         self.precisions_init = precisions_init\n@@ -185,6 +186,10 @@ class GaussianMixture(BaseMixture):\n             cov_params = self.n_components\n         mean_params = n_features * self.n_components\n         return int(cov_params + mean_params + self.n_components - 1)\n+    def fit_predict(self, X, y=None):\n+        self.fit(X, y)\n+        self.labels_ = self.predict(X)\n+        return self.labels_\n     def bic(self, X):\n         return -2 * self.score(X) * X.shape[0] + self._n_parameters() * np.log(X.shape[0])\n     def aic(self, X):\n", "normalized_occurrence": 1}
