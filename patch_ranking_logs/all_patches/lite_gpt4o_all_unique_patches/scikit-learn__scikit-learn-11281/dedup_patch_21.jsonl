{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex 67ca063..871bfb5 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -580,7 +580,7 @@ class GaussianMixture(BaseMixture):\n         inference.\n     \"\"\"\n \n-    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,\n+    def __init__(self, n_clusters=1, covariance_type='full', tol=1e-3,\n                  reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',\n                  weights_init=None, means_init=None, precisions_init=None,\n                  random_state=None, warm_start=False,\n@@ -592,6 +592,7 @@ class GaussianMixture(BaseMixture):\n             verbose=verbose, verbose_interval=verbose_interval)\n \n         self.covariance_type = covariance_type\n+        self.n_clusters = n_clusters\n         self.weights_init = weights_init\n         self.means_init = means_init\n         self.precisions_init = precisions_init\n@@ -634,8 +635,8 @@ class GaussianMixture(BaseMixture):\n             X, resp, self.reg_covar, self.covariance_type)\n         weights /= n_samples\n \n-        self.weights_ = (weights if self.weights_init is None\n-                         else self.weights_init)\n+        self.weights_ = (weights if self.weights_init is None else self.weights_init)\n+        self.labels_ = np.argmax(resp, axis=1)\n         self.means_ = means if self.means_init is None else self.means_init\n \n         if self.precisions_init is None:\n@@ -652,7 +653,9 @@ class GaussianMixture(BaseMixture):\n         else:\n             self.precisions_cholesky_ = self.precisions_init\n \n-    def _m_step(self, X, log_resp):\n+    def fit_predict(self, X, y=None):\n+        self.fit(X)\n+        return self.labels_\n         \"\"\"M step.\n \n         Parameters\n@@ -747,4 +750,4 @@ class GaussianMixture(BaseMixture):\n         aic : float\n             The lower the better.\n         \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex ff762d7..0c53336 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -116,9 +116,10 @@ def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n         log_prob = np.sum(means ** 2, 1) * precisions - 2 * np.dot(X, means.T * precisions) + np.outer(row_norms(X, squared=True), precisions)\n     return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n class GaussianMixture(BaseMixture):\n-    def __init__(self, n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10):\n+    def __init__(self, n_clusters=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10):\n         super(GaussianMixture, self).__init__(n_components=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval)\n         self.covariance_type = covariance_type\n+        self.n_clusters = n_clusters\n         self.weights_init = weights_init\n         self.means_init = means_init\n         self.precisions_init = precisions_init\n@@ -137,6 +138,7 @@ class GaussianMixture(BaseMixture):\n         weights, means, covariances = _estimate_gaussian_parameters(X, resp, self.reg_covar, self.covariance_type)\n         weights /= n_samples\n         self.weights_ = weights if self.weights_init is None else self.weights_init\n+        self.labels_ = np.argmax(resp, axis=1)\n         self.means_ = means if self.means_init is None else self.means_init\n         if self.precisions_init is None:\n             self.covariances_ = covariances\n@@ -147,7 +149,9 @@ class GaussianMixture(BaseMixture):\n             self.precisions_cholesky_ = linalg.cholesky(self.precisions_init, lower=True)\n         else:\n             self.precisions_cholesky_ = self.precisions_init\n-    def _m_step(self, X, log_resp):\n+    def fit_predict(self, X, y=None):\n+        self.fit(X)\n+        return self.labels_\n         n_samples, _ = X.shape\n         self.weights_, self.means_, self.covariances_ = _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar, self.covariance_type)\n         self.weights_ /= n_samples\n", "normalized_occurrence": 1}
