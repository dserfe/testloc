{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-4094", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 1e17b93..0634b48 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3462,11 +3462,14 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             # must be list for .expand_dims\n             expand_dims = list(expand_dims)\n \n-            return (\n-                val.assign_coords(**assign_coords)\n-                .expand_dims(expand_dims)\n-                .stack({new_dim: (variable_dim,) + stacking_dims})\n-            )\n+            if len(stacking_dims) == 0:\n+                return val.assign_coords(**assign_coords).expand_dims(variable_dim)\n+            else:\n+                return (\n+                    val.assign_coords(**assign_coords)\n+                    .expand_dims(expand_dims)\n+                    .stack({new_dim: (variable_dim,) + stacking_dims})\n+                )\n \n         # concatenate the arrays\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n@@ -3489,8 +3492,11 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n \n     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> \"Dataset\":\n         index = self.get_index(dim)\n-        index = remove_unused_levels_categories(index)\n-        full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n+        if isinstance(index, pd.MultiIndex):\n+            index = remove_unused_levels_categories(index)\n+            full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n+        else:\n+            full_idx = index\n \n         # take a shortcut in case the MultiIndex was not modified.\n         if index.equals(full_idx):\n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 21ef2a4..951a86f 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1153,7 +1153,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             expand_dims = set(stacking_dims).difference(set(val.dims))\n             expand_dims.add(variable_dim)\n             expand_dims = list(expand_dims)\n-            return val.assign_coords(**assign_coords).expand_dims(expand_dims).stack({new_dim: (variable_dim,) + stacking_dims})\n+            if len(stacking_dims) == 0:\n+                return val.assign_coords(**assign_coords).expand_dims(variable_dim)\n+            else:\n+                return val.assign_coords(**assign_coords).expand_dims(expand_dims).stack({new_dim: (variable_dim,) + stacking_dims})\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n         data_array = xr.concat(stackable_vars, dim=new_dim)\n         idx = data_array.indexes[new_dim]\n@@ -1165,8 +1168,11 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         return data_array\n     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> 'Dataset':\n         index = self.get_index(dim)\n-        index = remove_unused_levels_categories(index)\n-        full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n+        if isinstance(index, pd.MultiIndex):\n+            index = remove_unused_levels_categories(index)\n+            full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n+        else:\n+            full_idx = index\n         if index.equals(full_idx):\n             obj = self\n         else:\n", "normalized_occurrence": 1}
