{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex 67ca063..aa77e23 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -620,6 +620,7 @@ class GaussianMixture(BaseMixture):\n                                                      n_features)\n \n     def _initialize(self, X, resp):\n+        self.labels_ = None\n         \"\"\"Initialization of the Gaussian mixture parameters.\n \n         Parameters\n@@ -720,6 +721,67 @@ class GaussianMixture(BaseMixture):\n         mean_params = n_features * self.n_components\n         return int(cov_params + mean_params + self.n_components - 1)\n \n+    def fit(self, X, y=None):\n+        \"\"\"Estimate model parameters with the EM algorithm.\n+\n+        The method fits the model `n_init` times and sets the parameters with\n+        which the model has the largest likelihood or lower bound. Within each\n+        trial, the method iterates between E-step and M-step for\n+        `max_iter` times until the change of likelihood or lower bound is\n+        less than `tol`, otherwise, a `ConvergenceWarning` is raised. If\n+        `warm_start` is `True`, then `n_init` is ignored and a single\n+        initialization is performed upon the first call. Upon consecutive\n+        calls, training starts where it left off.\n+\n+        Parameters\n+        ----------\n+        X : array-like of shape (n_samples, n_features)\n+            The input data.\n+\n+        y : Ignored\n+\n+        Returns\n+        -------\n+        self\n+        \"\"\"\n+        self.fit_predict(X, y)\n+        return self\n+\n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model to X and then predict the labels for the data samples in X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            The input data.\n+\n+        y : Ignored\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        self.fit(X)\n+        return self.predict(X)\n+\n+    def predict(self, X):\n+        \"\"\"Predict the labels for the data samples in X using trained model.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            The input data.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        self._check_is_fitted()\n+        log_prob_norm = self._estimate_log_prob_resp(X)[0]\n+        return log_prob_norm.argmax(axis=1)\n+\n     def bic(self, X):\n         \"\"\"Bayesian information criterion for the current model on the input X.\n \n@@ -747,4 +809,4 @@ class GaussianMixture(BaseMixture):\n         aic : float\n             The lower the better.\n         \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex ff762d7..e06cfe7 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -133,6 +133,7 @@ class GaussianMixture(BaseMixture):\n         if self.precisions_init is not None:\n             self.precisions_init = _check_precisions(self.precisions_init, self.covariance_type, self.n_components, n_features)\n     def _initialize(self, X, resp):\n+        self.labels_ = None\n         n_samples, _ = X.shape\n         weights, means, covariances = _estimate_gaussian_parameters(X, resp, self.reg_covar, self.covariance_type)\n         weights /= n_samples\n@@ -185,6 +186,16 @@ class GaussianMixture(BaseMixture):\n             cov_params = self.n_components\n         mean_params = n_features * self.n_components\n         return int(cov_params + mean_params + self.n_components - 1)\n+    def fit(self, X, y=None):\n+        self.fit_predict(X, y)\n+        return self\n+    def fit_predict(self, X, y=None):\n+        self.fit(X)\n+        return self.predict(X)\n+    def predict(self, X):\n+        self._check_is_fitted()\n+        log_prob_norm = self._estimate_log_prob_resp(X)[0]\n+        return log_prob_norm.argmax(axis=1)\n     def bic(self, X):\n         return -2 * self.score(X) * X.shape[0] + self._n_parameters() * np.log(X.shape[0])\n     def aic(self, X):\n", "normalized_occurrence": 1}
