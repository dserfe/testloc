{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13328", "model_patch": "diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex 774a22b..4365a0c 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -56,6 +56,10 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n     w = w[:n_features]\n     n_samples = np.sum(sample_weight)\n \n+    # Convert boolean X to float if necessary\n+    if X.dtype == bool:\n+        X = X.astype(float)\n+\n     # Calculate the values where |y - X'w -c / sigma| > epsilon\n     # The values above this threshold are outliers.\n     linear_loss = y - safe_sparse_dot(X, w)\n@@ -77,7 +81,7 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n     outlier_loss = (2. * epsilon * np.sum(outliers_sw * outliers) -\n                     sigma * n_sw_outliers * epsilon ** 2)\n \n-    # Calculate the quadratic loss due to the non-outliers.-\n+    # Calculate the quadratic loss due to the non-outliers.\n     # This is equal to |(y - X'w - c)**2 / sigma**2| * sigma\n     non_outliers = linear_loss[~outliers_mask]\n     weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex 81acf45..de4d106 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -14,6 +14,8 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n     sigma = w[-1]\n     w = w[:n_features]\n     n_samples = np.sum(sample_weight)\n+    if X.dtype == bool:\n+        X = X.astype(float)\n     linear_loss = y - safe_sparse_dot(X, w)\n     if fit_intercept:\n         linear_loss -= intercept\n", "normalized_occurrence": 1}
