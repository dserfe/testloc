{"model_name_or_path": "agentless_claude", "instance_id": "scikit-learn__scikit-learn-25931", "model_patch": "diff --git a/sklearn/ensemble/_iforest.py b/sklearn/ensemble/_iforest.py\nindex d995c25..96318b2 100644\n--- a/sklearn/ensemble/_iforest.py\n+++ b/sklearn/ensemble/_iforest.py\n@@ -345,10 +345,75 @@ class IsolationForest(OutlierMixin, BaseBagging):\n             return self\n \n         # else, define offset_ wrt contamination parameter\n-        self.offset_ = np.percentile(self.score_samples(X), 100.0 * self.contamination)\n+        # Using the same X as in fit, so no need to validate features names again\n+        scores = self.score_samples(X, check_feature_names=False)\n+        self.offset_ = np.percentile(scores, 100.0 * self.contamination)\n \n         return self\n \n+    def score_samples(self, X, check_feature_names=True):\n+        \"\"\"\n+        Opposite of the anomaly score defined in the original paper.\n+\n+        The anomaly score of an input sample is computed as\n+        the mean anomaly score of all trees in the forest.\n+\n+        Parameters\n+        ----------\n+        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n+            The input samples.\n+        check_feature_names : bool, default=True\n+            Whether to check feature names during data validation.\n+\n+        Returns\n+        -------\n+        scores : ndarray of shape (n_samples,)\n+            The anomaly score of the input samples.\n+            The lower, the more abnormal. Negative scores represent outliers,\n+            positive scores represent inliers.\n+        \"\"\"\n+        # Check data\n+        check_params = {\"accept_sparse\": [\"csc\"], \"dtype\": tree_dtype}\n+        if not check_feature_names:\n+            check_params[\"force_all_finite\"] = \"allow-nan\"\n+        X = self._validate_data(X, reset=False, **check_params)\n+\n+        # Initialize output array\n+        scores = np.zeros(X.shape[0])\n+\n+        for i, (estimator, average_path_length, decision_path_lengths) in enumerate(\n+            zip(\n+                self.estimators_,\n+                self._average_path_length_per_tree,\n+                self._decision_path_lengths,\n+            )\n+        ):\n+            current_score = estimator.decision_path(X).toarray().sum(axis=1)\n+\n+            # Add the average path length of unseen nodes (those not in the\n+            # decision path) which are the complement of the observed path lengths\n+            current_score += (\n+                average_path_length\n+                - np.take(decision_path_lengths, estimator.tree_.apply(X))\n+            )\n+            scores += current_score\n+\n+        scores /= len(self.estimators_)\n+\n+        # Transform score according to [1] Eq. (2):\n+        # 2 ** -E[h(x)]/c(n) is approximately equal to 2 ** -E[h(x)]/E[h(T)]\n+        divisor = np.zeros(scores.shape)\n+        n_samples = X.shape[0]\n+\n+        for average_path_length in self._average_path_length_per_tree:\n+            divisor += average_path_length(n_samples)\n+        divisor /= len(self.estimators_)\n+        scores = 2 ** (-scores / divisor)\n+\n+        # Return the opposite of the scores as bigger is better (here less\n+        # abnormal) and add offset_\n+        return -scores\n+\n     def predict(self, X):\n         \"\"\"\n         Predict if a particular sample is an outlier or not.\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/ensemble/_iforest.py b/sklearn/ensemble/_iforest.py\nindex f9e64b9..fe68f5b 100644\n--- a/sklearn/ensemble/_iforest.py\n+++ b/sklearn/ensemble/_iforest.py\n@@ -46,8 +46,27 @@ class IsolationForest(OutlierMixin, BaseBagging):\n         if self.contamination == 'auto':\n             self.offset_ = -0.5\n             return self\n-        self.offset_ = np.percentile(self.score_samples(X), 100.0 * self.contamination)\n+        scores = self.score_samples(X, check_feature_names=False)\n+        self.offset_ = np.percentile(scores, 100.0 * self.contamination)\n         return self\n+    def score_samples(self, X, check_feature_names=True):\n+        check_params = {'accept_sparse': ['csc'], 'dtype': tree_dtype}\n+        if not check_feature_names:\n+            check_params['force_all_finite'] = 'allow-nan'\n+        X = self._validate_data(X, reset=False, **check_params)\n+        scores = np.zeros(X.shape[0])\n+        for i, (estimator, average_path_length, decision_path_lengths) in enumerate(zip(self.estimators_, self._average_path_length_per_tree, self._decision_path_lengths)):\n+            current_score = estimator.decision_path(X).toarray().sum(axis=1)\n+            current_score += average_path_length - np.take(decision_path_lengths, estimator.tree_.apply(X))\n+            scores += current_score\n+        scores /= len(self.estimators_)\n+        divisor = np.zeros(scores.shape)\n+        n_samples = X.shape[0]\n+        for average_path_length in self._average_path_length_per_tree:\n+            divisor += average_path_length(n_samples)\n+        divisor /= len(self.estimators_)\n+        scores = 2 ** (-scores / divisor)\n+        return -scores\n     def predict(self, X):\n         check_is_fitted(self)\n         decision_func = self.decision_function(X)\n", "normalized_occurrence": 1}
