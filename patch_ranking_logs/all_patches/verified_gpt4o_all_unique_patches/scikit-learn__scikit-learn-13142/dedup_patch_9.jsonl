{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13142", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a853e4a..5eefff9 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -188,10 +188,10 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         -------\n         self\n         \"\"\"\n-        self.fit_predict(X, y)\n+        self.fit(X, y)\n         return self\n \n-    def fit_predict(self, X, y=None):\n+    def predict(self, X):\n         \"\"\"Estimate model parameters using X and predict the labels for X.\n \n         The method fits the model n_init times and sets the parameters with\n@@ -257,11 +257,6 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. '\n                           'Try different init parameters, '\n@@ -273,6 +268,11 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n+        # Always do a final e-step to guarantee that the labels returned by\n+        # fit_predict(X) are always consistent with fit(X).predict(X)\n+        # for any value of max_iter and tol (and any random_state).\n+        _, log_resp = self._e_step(X)\n+\n         return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 9518b16..4e273f6 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -62,9 +62,9 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n     def _initialize(self, X, resp):\n         pass\n     def fit(self, X, y=None):\n-        self.fit_predict(X, y)\n+        self.fit(X, y)\n         return self\n-    def fit_predict(self, X, y=None):\n+    def predict(self, X):\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n         self._check_initial_parameters(X)\n         do_init = not (self.warm_start and hasattr(self, 'converged_'))\n@@ -93,12 +93,12 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n                 max_lower_bound = lower_bound\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n-        _, log_resp = self._e_step(X)\n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.' % (init + 1), ConvergenceWarning)\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n+        _, log_resp = self._e_step(X)\n         return log_resp.argmax(axis=1)\n     def _e_step(self, X):\n         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n", "normalized_occurrence": 1}
