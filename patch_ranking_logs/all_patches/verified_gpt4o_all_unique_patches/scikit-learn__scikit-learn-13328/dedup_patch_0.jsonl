{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13328", "model_patch": "diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex 774a22b..01db6fc 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -77,7 +77,9 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n     outlier_loss = (2. * epsilon * np.sum(outliers_sw * outliers) -\n                     sigma * n_sw_outliers * epsilon ** 2)\n \n-    # Calculate the quadratic loss due to the non-outliers.-\n+    # Calculate the quadratic loss due to the non-outliers.\n+    non_outliers = linear_loss[~outliers_mask].astype(float)\n+    weighted_non_outliers = (sample_weight[~outliers_mask] * non_outliers).astype(float)\n     # This is equal to |(y - X'w - c)**2 / sigma**2| * sigma\n     non_outliers = linear_loss[~outliers_mask]\n     weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers\n@@ -90,16 +92,16 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n         grad = np.zeros(n_features + 1)\n \n     # Gradient due to the squared loss.\n-    X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers)\n+    X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers).astype(float)\n     grad[:n_features] = (\n         2. / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers))\n \n     # Gradient due to the linear loss.\n-    signed_outliers = np.ones_like(outliers)\n+    signed_outliers = np.ones_like(outliers, dtype=float)\n     signed_outliers_mask = linear_loss[outliers_mask] < 0\n     signed_outliers[signed_outliers_mask] = -1.0\n     X_outliers = axis0_safe_slice(X, outliers_mask, num_outliers)\n-    sw_outliers = sample_weight[outliers_mask] * signed_outliers\n+    sw_outliers = (sample_weight[outliers_mask] * signed_outliers).astype(float)\n     grad[:n_features] -= 2. * epsilon * (\n         safe_sparse_dot(sw_outliers, X_outliers))\n \n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex 81acf45..421b438 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -25,6 +25,8 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n     outliers_sw = sample_weight[outliers_mask]\n     n_sw_outliers = np.sum(outliers_sw)\n     outlier_loss = 2.0 * epsilon * np.sum(outliers_sw * outliers) - sigma * n_sw_outliers * epsilon ** 2\n+    non_outliers = linear_loss[~outliers_mask].astype(float)\n+    weighted_non_outliers = (sample_weight[~outliers_mask] * non_outliers).astype(float)\n     non_outliers = linear_loss[~outliers_mask]\n     weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers\n     weighted_loss = np.dot(weighted_non_outliers.T, non_outliers)\n@@ -33,13 +35,13 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n         grad = np.zeros(n_features + 2)\n     else:\n         grad = np.zeros(n_features + 1)\n-    X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers)\n+    X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers).astype(float)\n     grad[:n_features] = 2.0 / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers)\n-    signed_outliers = np.ones_like(outliers)\n+    signed_outliers = np.ones_like(outliers, dtype=float)\n     signed_outliers_mask = linear_loss[outliers_mask] < 0\n     signed_outliers[signed_outliers_mask] = -1.0\n     X_outliers = axis0_safe_slice(X, outliers_mask, num_outliers)\n-    sw_outliers = sample_weight[outliers_mask] * signed_outliers\n+    sw_outliers = (sample_weight[outliers_mask] * signed_outliers).astype(float)\n     grad[:n_features] -= 2.0 * epsilon * safe_sparse_dot(sw_outliers, X_outliers)\n     grad[:n_features] += alpha * 2.0 * w\n     grad[-1] = n_samples\n", "normalized_occurrence": 1}
