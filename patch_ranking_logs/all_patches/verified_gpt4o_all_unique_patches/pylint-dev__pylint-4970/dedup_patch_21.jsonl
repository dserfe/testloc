{"model_name_or_path": "agentless", "instance_id": "pylint-dev__pylint-4970", "model_patch": "diff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex 79a566f..6d7e4e7 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -396,11 +396,12 @@ class Similar:\n         \"\"\"compute similarities in appended files\"\"\"\n         no_duplicates: Dict[int, List[Set[LinesChunkLimits_T]]] = defaultdict(list)\n \n-        for commonality in self._iter_sims():\n-            num = commonality.cmn_lines_nb\n-            lineset1 = commonality.fst_lset\n-            start_line_1 = commonality.fst_file_start\n-            end_line_1 = commonality.fst_file_end\n+        if self.min_lines > 0:\n+            for commonality in self._iter_sims():\n+                num = commonality.cmn_lines_nb\n+                lineset1 = commonality.fst_lset\n+                start_line_1 = commonality.fst_file_start\n+                end_line_1 = commonality.fst_file_end\n             lineset2 = commonality.snd_lset\n             start_line_2 = commonality.snd_file_start\n             end_line_2 = commonality.snd_file_end\n@@ -531,9 +532,10 @@ class Similar:\n         \"\"\"iterate on similarities among all files, by making a cartesian\n         product\n         \"\"\"\n-        for idx, lineset in enumerate(self.linesets[:-1]):\n-            for lineset2 in self.linesets[idx + 1 :]:\n-                yield from self._find_common(lineset, lineset2)\n+        if self.min_lines > 0:\n+            for idx, lineset in enumerate(self.linesets[:-1]):\n+                for lineset2 in self.linesets[idx + 1 :]:\n+                    yield from self._find_common(lineset, lineset2)\n \n     def get_map_data(self):\n         \"\"\"Returns the data we can use for a map/reduce process\n@@ -830,24 +832,25 @@ class SimilarChecker(BaseChecker, Similar, MapReduceMixin):\n \n     def close(self):\n         \"\"\"compute and display similarities on closing (i.e. end of parsing)\"\"\"\n-        total = sum(len(lineset) for lineset in self.linesets)\n-        duplicated = 0\n-        stats = self.stats\n-        for num, couples in self._compute_sims():\n-            msg = []\n-            lineset = start_line = end_line = None\n-            for lineset, start_line, end_line in couples:\n-                msg.append(f\"=={lineset.name}:[{start_line}:{end_line}]\")\n-            msg.sort()\n-\n-            if lineset:\n-                for line in lineset.real_lines[start_line:end_line]:\n-                    msg.append(line.rstrip())\n-\n-            self.add_message(\"R0801\", args=(len(couples), \"\\n\".join(msg)))\n-            duplicated += num * (len(couples) - 1)\n-        stats[\"nb_duplicated_lines\"] = duplicated\n-        stats[\"percent_duplicated_lines\"] = total and duplicated * 100.0 / total\n+        if self.min_lines > 0:\n+            total = sum(len(lineset) for lineset in self.linesets)\n+            duplicated = 0\n+            stats = self.stats\n+            for num, couples in self._compute_sims():\n+                msg = []\n+                lineset = start_line = end_line = None\n+                for lineset, start_line, end_line in couples:\n+                    msg.append(f\"=={lineset.name}:[{start_line}:{end_line}]\")\n+                msg.sort()\n+\n+                if lineset:\n+                    for line in lineset.real_lines[start_line:end_line]:\n+                        msg.append(line.rstrip())\n+\n+                self.add_message(\"R0801\", args=(len(couples), \"\\n\".join(msg)))\n+                duplicated += num * (len(couples) - 1)\n+            stats[\"nb_duplicated_lines\"] = duplicated\n+            stats[\"percent_duplicated_lines\"] = total and duplicated * 100.0 / total\n \n     def get_map_data(self):\n         \"\"\"Passthru override\"\"\"\n", "occurrence": 1, "normalized_patch": "diff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex 0997b2e..d7f6db6 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -141,11 +141,12 @@ class Similar:\n         self._display_sims(self._compute_sims())\n     def _compute_sims(self) -> List[Tuple[int, Set[LinesChunkLimits_T]]]:\n         no_duplicates: Dict[int, List[Set[LinesChunkLimits_T]]] = defaultdict(list)\n-        for commonality in self._iter_sims():\n-            num = commonality.cmn_lines_nb\n-            lineset1 = commonality.fst_lset\n-            start_line_1 = commonality.fst_file_start\n-            end_line_1 = commonality.fst_file_end\n+        if self.min_lines > 0:\n+            for commonality in self._iter_sims():\n+                num = commonality.cmn_lines_nb\n+                lineset1 = commonality.fst_lset\n+                start_line_1 = commonality.fst_file_start\n+                end_line_1 = commonality.fst_file_end\n             lineset2 = commonality.snd_lset\n             start_line_2 = commonality.snd_file_start\n             end_line_2 = commonality.snd_file_end\n@@ -210,9 +211,10 @@ class Similar:\n             if eff_cmn_nb > self.min_lines:\n                 yield com\n     def _iter_sims(self) -> Generator[Commonality, None, None]:\n-        for idx, lineset in enumerate(self.linesets[:-1]):\n-            for lineset2 in self.linesets[idx + 1:]:\n-                yield from self._find_common(lineset, lineset2)\n+        if self.min_lines > 0:\n+            for idx, lineset in enumerate(self.linesets[:-1]):\n+                for lineset2 in self.linesets[idx + 1:]:\n+                    yield from self._find_common(lineset, lineset2)\n     def get_map_data(self):\n         return self.linesets\n     def combine_mapreduce_data(self, linesets_collection):\n@@ -321,22 +323,23 @@ class SimilarChecker(BaseChecker, Similar, MapReduceMixin):\n         with node.stream() as stream:\n             self.append_stream(self.linter.current_name, stream, node.file_encoding)\n     def close(self):\n-        total = sum((len(lineset) for lineset in self.linesets))\n-        duplicated = 0\n-        stats = self.stats\n-        for num, couples in self._compute_sims():\n-            msg = []\n-            lineset = start_line = end_line = None\n-            for lineset, start_line, end_line in couples:\n-                msg.append(f'=={lineset.name}:[{start_line}:{end_line}]')\n-            msg.sort()\n-            if lineset:\n-                for line in lineset.real_lines[start_line:end_line]:\n-                    msg.append(line.rstrip())\n-            self.add_message('R0801', args=(len(couples), '\\n'.join(msg)))\n-            duplicated += num * (len(couples) - 1)\n-        stats['nb_duplicated_lines'] = duplicated\n-        stats['percent_duplicated_lines'] = total and duplicated * 100.0 / total\n+        if self.min_lines > 0:\n+            total = sum((len(lineset) for lineset in self.linesets))\n+            duplicated = 0\n+            stats = self.stats\n+            for num, couples in self._compute_sims():\n+                msg = []\n+                lineset = start_line = end_line = None\n+                for lineset, start_line, end_line in couples:\n+                    msg.append(f'=={lineset.name}:[{start_line}:{end_line}]')\n+                msg.sort()\n+                if lineset:\n+                    for line in lineset.real_lines[start_line:end_line]:\n+                        msg.append(line.rstrip())\n+                self.add_message('R0801', args=(len(couples), '\\n'.join(msg)))\n+                duplicated += num * (len(couples) - 1)\n+            stats['nb_duplicated_lines'] = duplicated\n+            stats['percent_duplicated_lines'] = total and duplicated * 100.0 / total\n     def get_map_data(self):\n         return Similar.get_map_data(self)\n     def reduce_map_data(self, linter, data):\n", "normalized_occurrence": 1}
