{"model_name_or_path": "agentless", "instance_id": "pylint-dev__pylint-4970", "model_patch": "diff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex 79a566f..6eda406 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -475,15 +475,18 @@ class Similar:\n         hash_to_index_2: HashToIndex_T\n         index_to_lines_1: IndexToLines_T\n         index_to_lines_2: IndexToLines_T\n-        hash_to_index_1, index_to_lines_1 = hash_lineset(lineset1, self.min_lines)\n-        hash_to_index_2, index_to_lines_2 = hash_lineset(lineset2, self.min_lines)\n+        if self.min_lines > 0:\n+            hash_to_index_1, index_to_lines_1 = hash_lineset(lineset1, self.min_lines)\n+            hash_to_index_2, index_to_lines_2 = hash_lineset(lineset2, self.min_lines)\n \n-        hash_1: FrozenSet[LinesChunk] = frozenset(hash_to_index_1.keys())\n-        hash_2: FrozenSet[LinesChunk] = frozenset(hash_to_index_2.keys())\n+            hash_1: FrozenSet[LinesChunk] = frozenset(hash_to_index_1.keys())\n+            hash_2: FrozenSet[LinesChunk] = frozenset(hash_to_index_2.keys())\n \n-        common_hashes: Iterable[LinesChunk] = sorted(\n-            hash_1 & hash_2, key=lambda m: hash_to_index_1[m][0]\n-        )\n+            common_hashes: Iterable[LinesChunk] = sorted(\n+                hash_1 & hash_2, key=lambda m: hash_to_index_1[m][0]\n+            )\n+        else:\n+            common_hashes = []\n \n         # all_couples is a dict that links the couple of indices in both linesets that mark the beginning of\n         # successive common lines, to the corresponding starting and ending number lines in both files\n@@ -531,9 +534,10 @@ class Similar:\n         \"\"\"iterate on similarities among all files, by making a cartesian\n         product\n         \"\"\"\n-        for idx, lineset in enumerate(self.linesets[:-1]):\n-            for lineset2 in self.linesets[idx + 1 :]:\n-                yield from self._find_common(lineset, lineset2)\n+        if self.min_lines > 0:\n+            for idx, lineset in enumerate(self.linesets[:-1]):\n+                for lineset2 in self.linesets[idx + 1 :]:\n+                    yield from self._find_common(lineset, lineset2)\n \n     def get_map_data(self):\n         \"\"\"Returns the data we can use for a map/reduce process\n", "occurrence": 1, "normalized_patch": "diff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex 0997b2e..3b289d5 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -189,11 +189,14 @@ class Similar:\n         hash_to_index_2: HashToIndex_T\n         index_to_lines_1: IndexToLines_T\n         index_to_lines_2: IndexToLines_T\n-        hash_to_index_1, index_to_lines_1 = hash_lineset(lineset1, self.min_lines)\n-        hash_to_index_2, index_to_lines_2 = hash_lineset(lineset2, self.min_lines)\n-        hash_1: FrozenSet[LinesChunk] = frozenset(hash_to_index_1.keys())\n-        hash_2: FrozenSet[LinesChunk] = frozenset(hash_to_index_2.keys())\n-        common_hashes: Iterable[LinesChunk] = sorted(hash_1 & hash_2, key=lambda m: hash_to_index_1[m][0])\n+        if self.min_lines > 0:\n+            hash_to_index_1, index_to_lines_1 = hash_lineset(lineset1, self.min_lines)\n+            hash_to_index_2, index_to_lines_2 = hash_lineset(lineset2, self.min_lines)\n+            hash_1: FrozenSet[LinesChunk] = frozenset(hash_to_index_1.keys())\n+            hash_2: FrozenSet[LinesChunk] = frozenset(hash_to_index_2.keys())\n+            common_hashes: Iterable[LinesChunk] = sorted(hash_1 & hash_2, key=lambda m: hash_to_index_1[m][0])\n+        else:\n+            common_hashes = []\n         all_couples: CplIndexToCplLines_T = {}\n         for c_hash in sorted(common_hashes, key=operator.attrgetter('_index')):\n             for indices_in_linesets in itertools.product(hash_to_index_1[c_hash], hash_to_index_2[c_hash]):\n@@ -210,9 +213,10 @@ class Similar:\n             if eff_cmn_nb > self.min_lines:\n                 yield com\n     def _iter_sims(self) -> Generator[Commonality, None, None]:\n-        for idx, lineset in enumerate(self.linesets[:-1]):\n-            for lineset2 in self.linesets[idx + 1:]:\n-                yield from self._find_common(lineset, lineset2)\n+        if self.min_lines > 0:\n+            for idx, lineset in enumerate(self.linesets[:-1]):\n+                for lineset2 in self.linesets[idx + 1:]:\n+                    yield from self._find_common(lineset, lineset2)\n     def get_map_data(self):\n         return self.linesets\n     def combine_mapreduce_data(self, linesets_collection):\n", "normalized_occurrence": 1}
