{"model_name_or_path": "agentless_claude", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 721709f..ef412c5 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -169,6 +169,38 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n         \"\"\"\n         pass\n \n+    @property\n+    def n_clusters(self):\n+        \"\"\"Number of clusters (alias for n_components).\"\"\"\n+        return self.n_components\n+\n+    @n_clusters.setter \n+    def n_clusters(self, value):\n+        self.n_components = value\n+\n+    def fit_predict(self, X, y=None):\n+        \"\"\"Estimate model parameters using X and predict the labels for X.\n+\n+        The method fits the model n_init times and sets the parameters with\n+        which the model has the largest likelihood or lower bound. Within each\n+        trial, the method iterates between E-step and M-step for `max_iter`\n+        times until the change of likelihood or lower bound is less than\n+        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n+        predicts the most probable label for the input data points.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        return self.fit(X).predict(X)\n+\n     def fit(self, X, y=None):\n         \"\"\"Estimate model parameters with the EM algorithm.\n \n@@ -239,7 +271,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n \n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n-\n+        \n+        # Store labels_ attribute\n+        self.labels_ = self.predict(X)\n+        \n         return self\n \n     def _e_step(self, X):\n@@ -500,4 +535,4 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n             print(\"Initialization converged: %s\" % self.converged_)\n         elif self.verbose >= 2:\n             print(\"Initialization converged: %s\\t time lapse %.5fs\\t ll %.5f\" %\n-                  (self.converged_, time() - self._init_prev_time, ll))\n+                  (self.converged_, time() - self._init_prev_time, ll))\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 71fc3c3..97592cf 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -63,6 +63,14 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     @abstractmethod\n     def _initialize(self, X, resp):\n         pass\n+    @property\n+    def n_clusters(self):\n+        return self.n_components\n+    @n_clusters.setter\n+    def n_clusters(self, value):\n+        self.n_components = value\n+    def fit_predict(self, X, y=None):\n+        return self.fit(X).predict(X)\n     def fit(self, X, y=None):\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n         self._check_initial_parameters(X)\n@@ -96,6 +104,7 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n             warnings.warn('Initialization %d did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.' % (init + 1), ConvergenceWarning)\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n         return self\n     def _e_step(self, X):\n         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n", "normalized_occurrence": 2}
