{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13328", "model_patch": "diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex 774a22b..d2280a9 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -90,7 +90,7 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n         grad = np.zeros(n_features + 1)\n \n     # Gradient due to the squared loss.\n-    X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers)\n+    X_non_outliers = axis0_safe_slice(X, np.logical_not(outliers_mask), n_non_outliers)\n     grad[:n_features] = (\n         2. / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers))\n \n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex 81acf45..256c58e 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -33,7 +33,7 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n         grad = np.zeros(n_features + 2)\n     else:\n         grad = np.zeros(n_features + 1)\n-    X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers)\n+    X_non_outliers = axis0_safe_slice(X, np.logical_not(outliers_mask), n_non_outliers)\n     grad[:n_features] = 2.0 / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers)\n     signed_outliers = np.ones_like(outliers)\n     signed_outliers_mask = linear_loss[outliers_mask] < 0\n", "normalized_occurrence": 1}
