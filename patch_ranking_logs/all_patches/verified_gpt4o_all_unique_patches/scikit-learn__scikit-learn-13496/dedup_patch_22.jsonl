{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13496", "model_patch": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 17608bc..f5b1a32 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -173,6 +173,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n+                 warm_start=False,\n                  verbose=0):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n@@ -187,9 +188,12 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n+            warm_start=warm_start,\n             verbose=verbose)\n+        self.warm_start = warm_start\n \n         self.behaviour = behaviour\n+        self.warm_start = warm_start\n         self.contamination = contamination\n \n     def _set_oob_score(self, X, y):\n@@ -212,6 +216,11 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             efficiency. Sparse matrices are also supported, use sparse\n             ``csc_matrix`` for maximum efficiency.\n \n+        warm_start : bool, optional (default=False)\n+            When set to ``True``, reuse the solution of the previous call to fit\n+            and add more estimators to the ensemble, otherwise, just fit a whole\n+            new forest. See :term:`the Glossary <warm_start>`.\n+\n         sample_weight : array-like, shape = [n_samples] or None\n             Sample weights. If None, then samples are equally weighted.\n \n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 3ca6fb5..8ca75fb 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -11,9 +11,11 @@ from .bagging import BaseBagging\n __all__ = ['IsolationForest']\n INTEGER_TYPES = (numbers.Integral, np.integer)\n class IsolationForest(BaseBagging, OutlierMixin):\n-    def __init__(self, n_estimators=100, max_samples='auto', contamination='legacy', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='old', random_state=None, verbose=0):\n-        super().__init__(base_estimator=ExtraTreeRegressor(max_features=1, splitter='random', random_state=random_state), bootstrap=bootstrap, bootstrap_features=False, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, n_jobs=n_jobs, random_state=random_state, verbose=verbose)\n+    def __init__(self, n_estimators=100, max_samples='auto', contamination='legacy', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='old', random_state=None, warm_start=False, verbose=0):\n+        super().__init__(base_estimator=ExtraTreeRegressor(max_features=1, splitter='random', random_state=random_state), bootstrap=bootstrap, bootstrap_features=False, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, n_jobs=n_jobs, random_state=random_state, warm_start=warm_start, verbose=verbose)\n+        self.warm_start = warm_start\n         self.behaviour = behaviour\n+        self.warm_start = warm_start\n         self.contamination = contamination\n     def _set_oob_score(self, X, y):\n         raise NotImplementedError('OOB score not supported by iforest')\n", "normalized_occurrence": 1}
