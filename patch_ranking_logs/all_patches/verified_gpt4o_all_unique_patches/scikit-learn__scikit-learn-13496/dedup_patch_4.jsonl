{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13496", "model_patch": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 17608bc..1a23592 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -147,6 +147,11 @@ class IsolationForest(BaseBagging, OutlierMixin):\n         ``offset_ = -0.5``, making the decision function independent from the\n         contamination parameter.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     Notes\n     -----\n     The implementation is based on an ensemble of ExtraTreeRegressor. The\n@@ -173,7 +178,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -189,6 +195,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             random_state=random_state,\n             verbose=verbose)\n \n+        self.warm_start = warm_start\n         self.behaviour = behaviour\n         self.contamination = contamination\n \n@@ -274,6 +281,10 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_samples = int(self.max_samples * X.shape[0])\n \n         self.max_samples_ = max_samples\n+\n+        # Update n_estimators if warm_start is True\n+        if self.warm_start and hasattr(self, \"estimators_\"):\n+            self.n_estimators += len(self.estimators_)\n         max_depth = int(np.ceil(np.log2(max(max_samples, 2))))\n         super()._fit(X, y, max_samples,\n                      max_depth=max_depth,\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 3ca6fb5..e55e0fd 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -11,8 +11,9 @@ from .bagging import BaseBagging\n __all__ = ['IsolationForest']\n INTEGER_TYPES = (numbers.Integral, np.integer)\n class IsolationForest(BaseBagging, OutlierMixin):\n-    def __init__(self, n_estimators=100, max_samples='auto', contamination='legacy', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='old', random_state=None, verbose=0):\n+    def __init__(self, n_estimators=100, max_samples='auto', contamination='legacy', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='old', random_state=None, verbose=0, warm_start=False):\n         super().__init__(base_estimator=ExtraTreeRegressor(max_features=1, splitter='random', random_state=random_state), bootstrap=bootstrap, bootstrap_features=False, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, n_jobs=n_jobs, random_state=random_state, verbose=verbose)\n+        self.warm_start = warm_start\n         self.behaviour = behaviour\n         self.contamination = contamination\n     def _set_oob_score(self, X, y):\n@@ -49,6 +50,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                 raise ValueError('max_samples must be in (0, 1], got %r' % self.max_samples)\n             max_samples = int(self.max_samples * X.shape[0])\n         self.max_samples_ = max_samples\n+        if self.warm_start and hasattr(self, 'estimators_'):\n+            self.n_estimators += len(self.estimators_)\n         max_depth = int(np.ceil(np.log2(max(max_samples, 2))))\n         super()._fit(X, y, max_samples, max_depth=max_depth, sample_weight=sample_weight)\n         if self.behaviour == 'old':\n", "normalized_occurrence": 1}
