{"model_name_or_path": "agentless_claude", "instance_id": "pylint-dev__pylint-4970", "model_patch": "diff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex 79a566f..910d540 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -471,6 +471,9 @@ class Similar:\n         Last regroups all successive couples in a bigger one. It allows to take into account common chunk of lines that have more\n         than the minimal number of successive lines required.\n         \"\"\"\n+        # Skip similarity checking if min_lines is 0 (disabled)\n+        if self.min_lines == 0:\n+            return\n         hash_to_index_1: HashToIndex_T\n         hash_to_index_2: HashToIndex_T\n         index_to_lines_1: IndexToLines_T\n@@ -524,7 +527,7 @@ class Similar:\n                 lineset1, start_index_1, lineset2, start_index_2, nb_common_lines\n             )\n \n-            if eff_cmn_nb > self.min_lines:\n+            if eff_cmn_nb >= self.min_lines:\n                 yield com\n \n     def _iter_sims(self) -> Generator[Commonality, None, None]:\n", "occurrence": 1, "normalized_patch": "diff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex 0997b2e..9edecf9 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -185,6 +185,8 @@ class Similar:\n         report += f'TOTAL lines={total_line_number} duplicates={duplicated_line_number} percent={duplicated_line_number * 100.0 / total_line_number:.2f}\\n'\n         return report\n     def _find_common(self, lineset1: 'LineSet', lineset2: 'LineSet') -> Generator[Commonality, None, None]:\n+        if self.min_lines == 0:\n+            return\n         hash_to_index_1: HashToIndex_T\n         hash_to_index_2: HashToIndex_T\n         index_to_lines_1: IndexToLines_T\n@@ -207,7 +209,7 @@ class Similar:\n             nb_common_lines = cmn_l.effective_cmn_lines_nb\n             com = Commonality(cmn_lines_nb=nb_common_lines, fst_lset=lineset1, fst_file_start=cmn_l.first_file.start, fst_file_end=cmn_l.first_file.end, snd_lset=lineset2, snd_file_start=cmn_l.second_file.start, snd_file_end=cmn_l.second_file.end)\n             eff_cmn_nb = filter_noncode_lines(lineset1, start_index_1, lineset2, start_index_2, nb_common_lines)\n-            if eff_cmn_nb > self.min_lines:\n+            if eff_cmn_nb >= self.min_lines:\n                 yield com\n     def _iter_sims(self) -> Generator[Commonality, None, None]:\n         for idx, lineset in enumerate(self.linesets[:-1]):\n", "normalized_occurrence": 1}
