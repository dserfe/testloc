{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13142", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a853e4a..149c7f5 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -188,7 +188,65 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         -------\n         self\n         \"\"\"\n-        self.fit_predict(X, y)\n+        X = _check_X(X, self.n_components, ensure_min_samples=2)\n+        self._check_initial_parameters(X)\n+\n+        # if we enable warm_start, we will have a unique initialisation\n+        do_init = not(self.warm_start and hasattr(self, 'converged_'))\n+        n_init = self.n_init if do_init else 1\n+\n+        max_lower_bound = -np.infty\n+        self.converged_ = False\n+\n+        random_state = check_random_state(self.random_state)\n+\n+        n_samples, _ = X.shape\n+        for init in range(n_init):\n+            self._print_verbose_msg_init_beg(init)\n+\n+            if do_init:\n+                self._initialize_parameters(X, random_state)\n+\n+            lower_bound = (-np.infty if do_init else self.lower_bound_)\n+\n+            for n_iter in range(1, self.max_iter + 1):\n+                prev_lower_bound = lower_bound\n+\n+                log_prob_norm, log_resp = self._e_step(X)\n+                self._m_step(X, log_resp)\n+                lower_bound = self._compute_lower_bound(\n+                    log_resp, log_prob_norm)\n+\n+                change = lower_bound - prev_lower_bound\n+                self._print_verbose_msg_iter_end(n_iter, change)\n+\n+                if abs(change) < self.tol:\n+                    self.converged_ = True\n+                    break\n+\n+            self._print_verbose_msg_init_end(lower_bound)\n+\n+            if lower_bound > max_lower_bound:\n+                max_lower_bound = lower_bound\n+                best_params = self._get_parameters()\n+                best_n_iter = n_iter\n+\n+        # Perform the final e-step to guarantee that the labels are consistent\n+        # for any value of max_iter and tol (and any random_state).\n+        _, log_resp = self._e_step(X)\n+\n+        if not self.converged_:\n+            warnings.warn('Initialization %d did not converge. '\n+                          'Try different init parameters, '\n+                          'or increase max_iter, tol '\n+                          'or check for degenerate data.'\n+                          % (init + 1), ConvergenceWarning)\n+\n+        self._set_parameters(best_params)\n+        self.n_iter_ = best_n_iter\n+        self.lower_bound_ = max_lower_bound\n+\n+        return self\n         return self\n \n     def fit_predict(self, X, y=None):\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 9518b16..82fff4f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -62,7 +62,41 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n     def _initialize(self, X, resp):\n         pass\n     def fit(self, X, y=None):\n-        self.fit_predict(X, y)\n+        X = _check_X(X, self.n_components, ensure_min_samples=2)\n+        self._check_initial_parameters(X)\n+        do_init = not (self.warm_start and hasattr(self, 'converged_'))\n+        n_init = self.n_init if do_init else 1\n+        max_lower_bound = -np.infty\n+        self.converged_ = False\n+        random_state = check_random_state(self.random_state)\n+        n_samples, _ = X.shape\n+        for init in range(n_init):\n+            self._print_verbose_msg_init_beg(init)\n+            if do_init:\n+                self._initialize_parameters(X, random_state)\n+            lower_bound = -np.infty if do_init else self.lower_bound_\n+            for n_iter in range(1, self.max_iter + 1):\n+                prev_lower_bound = lower_bound\n+                log_prob_norm, log_resp = self._e_step(X)\n+                self._m_step(X, log_resp)\n+                lower_bound = self._compute_lower_bound(log_resp, log_prob_norm)\n+                change = lower_bound - prev_lower_bound\n+                self._print_verbose_msg_iter_end(n_iter, change)\n+                if abs(change) < self.tol:\n+                    self.converged_ = True\n+                    break\n+            self._print_verbose_msg_init_end(lower_bound)\n+            if lower_bound > max_lower_bound:\n+                max_lower_bound = lower_bound\n+                best_params = self._get_parameters()\n+                best_n_iter = n_iter\n+        _, log_resp = self._e_step(X)\n+        if not self.converged_:\n+            warnings.warn('Initialization %d did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.' % (init + 1), ConvergenceWarning)\n+        self._set_parameters(best_params)\n+        self.n_iter_ = best_n_iter\n+        self.lower_bound_ = max_lower_bound\n+        return self\n         return self\n     def fit_predict(self, X, y=None):\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n", "normalized_occurrence": 2}
