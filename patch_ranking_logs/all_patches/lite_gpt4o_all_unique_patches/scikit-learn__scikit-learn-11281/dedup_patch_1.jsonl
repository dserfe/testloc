{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex 67ca063..10ef3f8 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -596,6 +596,25 @@ class GaussianMixture(BaseMixture):\n         self.means_init = means_init\n         self.precisions_init = precisions_init\n \n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model to X and predict the cluster labels for X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            The input data.\n+\n+        y : Ignored\n+            Not used, present here for API consistency by convention.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        self.fit(X)\n+        return self.predict(X)\n+\n     def _check_parameters(self, X):\n         \"\"\"Check the Gaussian mixture parameters are well defined.\"\"\"\n         _, n_features = X.shape\n@@ -619,6 +638,47 @@ class GaussianMixture(BaseMixture):\n                                                      self.n_components,\n                                                      n_features)\n \n+    def fit(self, X, y=None):\n+        \"\"\"Estimate model parameters with the EM algorithm.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            The input data.\n+\n+        y : Ignored\n+            Not used, present here for API consistency by convention.\n+\n+        Returns\n+        -------\n+        self\n+        \"\"\"\n+        self._check_parameters(X)\n+        self._initialize_parameters(X, self.random_state)\n+        self.converged_ = False\n+\n+        lower_bound = -np.infty\n+        self.n_iter_ = 0\n+\n+        for n_iter in range(1, self.max_iter + 1):\n+            prev_lower_bound = lower_bound\n+\n+            log_prob_norm, log_resp = self._e_step(X)\n+            self._m_step(X, log_resp)\n+            lower_bound = self._compute_lower_bound(log_resp, log_prob_norm)\n+\n+            change = lower_bound - prev_lower_bound\n+            if abs(change) < self.tol:\n+                self.converged_ = True\n+                break\n+\n+        self.n_iter_ = n_iter\n+        self.lower_bound_ = lower_bound\n+\n+        self.labels_ = self.predict(X)\n+\n+        return self\n+\n     def _initialize(self, X, resp):\n         \"\"\"Initialization of the Gaussian mixture parameters.\n \n@@ -747,4 +807,4 @@ class GaussianMixture(BaseMixture):\n         aic : float\n             The lower the better.\n         \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n", "occurrence": 1, "normalized_patch": "--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n+    def fit_predict(self, X, y=None):\n+        self.fit(X)\n+        return self.predict(X)\n+    def fit(self, X, y=None):\n+        self._check_parameters(X)\n+        self._initialize_parameters(X, self.random_state)\n+        self.converged_ = False\n+        lower_bound = -np.infty\n+        self.n_iter_ = 0\n+        for n_iter in range(1, self.max_iter + 1):\n+            prev_lower_bound = lower_bound\n+            log_prob_norm, log_resp = self._e_step(X)\n+            self._m_step(X, log_resp)\n+            lower_bound = self._compute_lower_bound(log_resp, log_prob_norm)\n+            change = lower_bound - prev_lower_bound\n+            if abs(change) < self.tol:\n+                self.converged_ = True\n+                break\n+        self.n_iter_ = n_iter\n+        self.lower_bound_ = lower_bound\n+        self.labels_ = self.predict(X)\n+        return self", "normalized_occurrence": 1}
