{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-4094", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 1e17b93..5012510 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3442,13 +3442,12 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         \"\"\"\n         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n \n-        for variable in self:\n+        for variable in self.data_vars:\n             dims = self[variable].dims\n-            dims_include_sample_dims = set(sample_dims) <= set(dims)\n-            if not dims_include_sample_dims:\n+            if not set(sample_dims).issubset(dims):\n                 raise ValueError(\n-                    \"All variables in the dataset must contain the \"\n-                    \"dimensions {}.\".format(dims)\n+                    f\"All variables in the dataset must contain the dimensions {sample_dims}. \"\n+                    f\"Variable '{variable}' does not.\"\n                 )\n \n         def ensure_stackable(val):\n@@ -3475,12 +3474,13 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         # coerce the levels of the MultiIndex to have the same type as the\n         # input dimensions. This code is messy, so it might be better to just\n         # input a dummy value for the singleton dimension.\n-        idx = data_array.indexes[new_dim]\n-        levels = [idx.levels[0]] + [\n-            level.astype(self[level.name].dtype) for level in idx.levels[1:]\n-        ]\n-        new_idx = idx.set_levels(levels)\n-        data_array[new_dim] = IndexVariable(new_dim, new_idx)\n+        if isinstance(data_array.indexes[new_dim], pd.MultiIndex):\n+            idx = data_array.indexes[new_dim]\n+            levels = [idx.levels[0]] + [\n+                level.astype(self[level.name].dtype) for level in idx.levels[1:]\n+            ]\n+            new_idx = idx.set_levels(levels)\n+            data_array[new_dim] = IndexVariable(new_dim, new_idx)\n \n         if name is not None:\n             data_array.name = name\n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 21ef2a4..b733e56 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1140,11 +1140,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         return result\n     def to_stacked_array(self, new_dim: Hashable, sample_dims: Sequence[Hashable], variable_dim: str='variable', name: Hashable=None) -> 'DataArray':\n         stacking_dims = tuple((dim for dim in self.dims if dim not in sample_dims))\n-        for variable in self:\n+        for variable in self.data_vars:\n             dims = self[variable].dims\n-            dims_include_sample_dims = set(sample_dims) <= set(dims)\n-            if not dims_include_sample_dims:\n-                raise ValueError('All variables in the dataset must contain the dimensions {}.'.format(dims))\n+            if not set(sample_dims).issubset(dims):\n+                raise ValueError(f\"All variables in the dataset must contain the dimensions {sample_dims}. Variable '{variable}' does not.\")\n         def ensure_stackable(val):\n             assign_coords = {variable_dim: val.name}\n             for dim in stacking_dims:\n@@ -1156,10 +1155,11 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             return val.assign_coords(**assign_coords).expand_dims(expand_dims).stack({new_dim: (variable_dim,) + stacking_dims})\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n         data_array = xr.concat(stackable_vars, dim=new_dim)\n-        idx = data_array.indexes[new_dim]\n-        levels = [idx.levels[0]] + [level.astype(self[level.name].dtype) for level in idx.levels[1:]]\n-        new_idx = idx.set_levels(levels)\n-        data_array[new_dim] = IndexVariable(new_dim, new_idx)\n+        if isinstance(data_array.indexes[new_dim], pd.MultiIndex):\n+            idx = data_array.indexes[new_dim]\n+            levels = [idx.levels[0]] + [level.astype(self[level.name].dtype) for level in idx.levels[1:]]\n+            new_idx = idx.set_levels(levels)\n+            data_array[new_dim] = IndexVariable(new_dim, new_idx)\n         if name is not None:\n             data_array.name = name\n         return data_array\n", "normalized_occurrence": 1}
