{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13142", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a853e4a..d02e1ab 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -273,6 +273,20 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n+        self._set_parameters(best_params)  # set the best found parameters\n+        _, log_resp = self._e_step(X)  # perform a final E-step with the best parameters\n+\n+        if not self.converged_:\n+            warnings.warn('Initialization %d did not converge. '\n+                          'Try different init parameters, '\n+                          'or increase max_iter, tol '\n+                          'or check for degenerate data.'\n+                          % (init + 1), ConvergenceWarning)\n+\n+        self._set_parameters(best_params)\n+        self.n_iter_ = best_n_iter\n+        self.lower_bound_ = max_lower_bound\n+\n         return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 9518b16..f9de843 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -99,6 +99,13 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n+        self._set_parameters(best_params)\n+        _, log_resp = self._e_step(X)\n+        if not self.converged_:\n+            warnings.warn('Initialization %d did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.' % (init + 1), ConvergenceWarning)\n+        self._set_parameters(best_params)\n+        self.n_iter_ = best_n_iter\n+        self.lower_bound_ = max_lower_bound\n         return log_resp.argmax(axis=1)\n     def _e_step(self, X):\n         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n", "normalized_occurrence": 1}
