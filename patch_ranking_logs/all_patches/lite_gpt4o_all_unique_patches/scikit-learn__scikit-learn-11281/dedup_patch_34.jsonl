{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex 67ca063..dec6653 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -586,10 +586,11 @@ class GaussianMixture(BaseMixture):\n                  random_state=None, warm_start=False,\n                  verbose=0, verbose_interval=10):\n         super(GaussianMixture, self).__init__(\n-            n_components=n_components, tol=tol, reg_covar=reg_covar,\n+            n_clusters=n_components, tol=tol, reg_covar=reg_covar,\n             max_iter=max_iter, n_init=n_init, init_params=init_params,\n             random_state=random_state, warm_start=warm_start,\n             verbose=verbose, verbose_interval=verbose_interval)\n+        self.n_components = n_components\n \n         self.covariance_type = covariance_type\n         self.weights_init = weights_init\n@@ -681,6 +682,14 @@ class GaussianMixture(BaseMixture):\n     def _compute_lower_bound(self, _, log_prob_norm):\n         return log_prob_norm\n \n+    @property\n+    def labels_(self):\n+        return self.predict(self.means_)\n+\n+    def fit_predict(self, X):\n+        self.fit(X)\n+        return self.predict(X)\n+\n     def _check_is_fitted(self):\n         check_is_fitted(self, ['weights_', 'means_', 'precisions_cholesky_'])\n \n@@ -747,4 +756,4 @@ class GaussianMixture(BaseMixture):\n         aic : float\n             The lower the better.\n         \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex ff762d7..0107a9e 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -117,7 +117,8 @@ def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n     return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n class GaussianMixture(BaseMixture):\n     def __init__(self, n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10):\n-        super(GaussianMixture, self).__init__(n_components=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval)\n+        super(GaussianMixture, self).__init__(n_clusters=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval)\n+        self.n_components = n_components\n         self.covariance_type = covariance_type\n         self.weights_init = weights_init\n         self.means_init = means_init\n@@ -158,6 +159,12 @@ class GaussianMixture(BaseMixture):\n         return np.log(self.weights_)\n     def _compute_lower_bound(self, _, log_prob_norm):\n         return log_prob_norm\n+    @property\n+    def labels_(self):\n+        return self.predict(self.means_)\n+    def fit_predict(self, X):\n+        self.fit(X)\n+        return self.predict(X)\n     def _check_is_fitted(self):\n         check_is_fitted(self, ['weights_', 'means_', 'precisions_cholesky_'])\n     def _get_parameters(self):\n", "normalized_occurrence": 1}
