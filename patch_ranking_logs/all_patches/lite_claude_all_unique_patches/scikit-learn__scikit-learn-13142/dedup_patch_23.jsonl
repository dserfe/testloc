{"model_name_or_path": "agentless_claude", "instance_id": "scikit-learn__scikit-learn-13142", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a853e4a..da5656f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -272,8 +272,10 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n-\n-        return log_resp.argmax(axis=1)\n+        \n+        # Compute responsibilities for the optimal parameters\n+        _, best_log_resp = self._estimate_log_prob_resp(X)\n+        return best_log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n         \"\"\"E step.\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 9518b16..2109ac4 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -99,7 +99,8 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n-        return log_resp.argmax(axis=1)\n+        _, best_log_resp = self._estimate_log_prob_resp(X)\n+        return best_log_resp.argmax(axis=1)\n     def _e_step(self, X):\n         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n         return (np.mean(log_prob_norm), log_resp)\n", "normalized_occurrence": 5}
