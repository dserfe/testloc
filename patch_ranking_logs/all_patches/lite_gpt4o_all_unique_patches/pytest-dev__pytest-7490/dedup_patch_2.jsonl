{"model_name_or_path": "agentless", "instance_id": "pytest-dev__pytest-7490", "model_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 1316da9..17001fe 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -246,13 +246,12 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n-\n     if not item.config.option.runxfail:\n+        xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n+    else:\n+        item._store[xfailed_key] = evaluate_xfail_marks(item)\n \n     yield\n \n@@ -276,10 +275,9 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n-    elif not rep.skipped and xfailed:\n+    if xfailed and not item.config.option.runxfail and not rep.skipped:\n         if call.excinfo:\n-            raises = xfailed.raises\n-            if raises is not None and not isinstance(call.excinfo.value, raises):\n+            if xfailed.raises and not isinstance(call.excinfo.value, xfailed.raises):\n                 rep.outcome = \"failed\"\n             else:\n                 rep.outcome = \"skipped\"\n@@ -312,4 +310,4 @@ def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str\n             return \"xfailed\", \"x\", \"XFAIL\"\n         elif report.passed:\n             return \"xpassed\", \"X\", \"XPASS\"\n-    return None\n+    return None\n", "occurrence": 1, "normalized_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex d85ae4c..552ce8b 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -129,12 +129,12 @@ def pytest_runtest_setup(item: Item) -> None:\n             xfail('[NOTRUN] ' + xfailed.reason)\n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n     if not item.config.option.runxfail:\n+        xfailed = evaluate_xfail_marks(item)\n         if xfailed and (not xfailed.run):\n             xfail('[NOTRUN] ' + xfailed.reason)\n+    else:\n+        item._store[xfailed_key] = evaluate_xfail_marks(item)\n     yield\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n@@ -154,10 +154,9 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = 'reason: ' + call.excinfo.value.msg\n         rep.outcome = 'skipped'\n-    elif not rep.skipped and xfailed:\n+    if xfailed and (not item.config.option.runxfail) and (not rep.skipped):\n         if call.excinfo:\n-            raises = xfailed.raises\n-            if raises is not None and (not isinstance(call.excinfo.value, raises)):\n+            if xfailed.raises and (not isinstance(call.excinfo.value, xfailed.raises)):\n                 rep.outcome = 'failed'\n             else:\n                 rep.outcome = 'skipped'\n", "normalized_occurrence": 1}
