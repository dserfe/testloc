{"model_name_or_path": "agentless_claude", "instance_id": "pytest-dev__pytest-7490", "model_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 1316da9..10cedef 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -249,6 +249,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     xfailed = item._store.get(xfailed_key, None)\n     if xfailed is None:\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+    \n+    yield\n+    \n+    # Re-evaluate after test in case markers were added during test\n+    item._store[xfailed_key] = evaluate_xfail_marks(item)\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n", "occurrence": 1, "normalized_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex d85ae4c..d690de4 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -132,6 +132,8 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     xfailed = item._store.get(xfailed_key, None)\n     if xfailed is None:\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+    yield\n+    item._store[xfailed_key] = evaluate_xfail_marks(item)\n     if not item.config.option.runxfail:\n         if xfailed and (not xfailed.run):\n             xfail('[NOTRUN] ' + xfailed.reason)\n", "normalized_occurrence": 1}
