{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex 67ca063..ecca735 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,7 +445,7 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n-    n_components : int, defaults to 1.\n+    n_clusters : int, defaults to 1.\n         The number of mixture components.\n \n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n@@ -586,7 +586,7 @@ class GaussianMixture(BaseMixture):\n                  random_state=None, warm_start=False,\n                  verbose=0, verbose_interval=10):\n         super(GaussianMixture, self).__init__(\n-            n_components=n_components, tol=tol, reg_covar=reg_covar,\n+            n_clusters=n_components, tol=tol, reg_covar=reg_covar,\n             max_iter=max_iter, n_init=n_init, init_params=init_params,\n             random_state=random_state, warm_start=warm_start,\n             verbose=verbose, verbose_interval=verbose_interval)\n@@ -596,7 +596,25 @@ class GaussianMixture(BaseMixture):\n         self.means_init = means_init\n         self.precisions_init = precisions_init\n \n-    def _check_parameters(self, X):\n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model to X and then predict the closest cluster each sample in X belongs to.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        y : Ignored\n+            Not used, present here for API consistency by convention.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component memberships.\n+        \"\"\"\n+        self.fit(X)\n+        return self.predict(X)\n         \"\"\"Check the Gaussian mixture parameters are well defined.\"\"\"\n         _, n_features = X.shape\n         if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:\n@@ -619,7 +637,28 @@ class GaussianMixture(BaseMixture):\n                                                      self.n_components,\n                                                      n_features)\n \n-    def _initialize(self, X, resp):\n+    def fit(self, X, y=None):\n+        \"\"\"Estimate model parameters with the EM algorithm.\n+\n+        A initialization step is performed before entering the\n+        expectation-maximization (EM) algorithm.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        y : Ignored\n+            Not used, present here for API consistency by convention.\n+\n+        Returns\n+        -------\n+        self\n+        \"\"\"\n+        self._fit(X, y)\n+        self.labels_ = self.predict(X)\n+        return self\n         \"\"\"Initialization of the Gaussian mixture parameters.\n \n         Parameters\n@@ -747,4 +786,4 @@ class GaussianMixture(BaseMixture):\n         aic : float\n             The lower the better.\n         \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex ff762d7..6627d51 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -117,12 +117,14 @@ def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n     return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n class GaussianMixture(BaseMixture):\n     def __init__(self, n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10):\n-        super(GaussianMixture, self).__init__(n_components=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval)\n+        super(GaussianMixture, self).__init__(n_clusters=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval)\n         self.covariance_type = covariance_type\n         self.weights_init = weights_init\n         self.means_init = means_init\n         self.precisions_init = precisions_init\n-    def _check_parameters(self, X):\n+    def fit_predict(self, X, y=None):\n+        self.fit(X)\n+        return self.predict(X)\n         _, n_features = X.shape\n         if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:\n             raise ValueError(\"Invalid value for 'covariance_type': %s 'covariance_type' should be in ['spherical', 'tied', 'diag', 'full']\" % self.covariance_type)\n@@ -132,7 +134,10 @@ class GaussianMixture(BaseMixture):\n             self.means_init = _check_means(self.means_init, self.n_components, n_features)\n         if self.precisions_init is not None:\n             self.precisions_init = _check_precisions(self.precisions_init, self.covariance_type, self.n_components, n_features)\n-    def _initialize(self, X, resp):\n+    def fit(self, X, y=None):\n+        self._fit(X, y)\n+        self.labels_ = self.predict(X)\n+        return self\n         n_samples, _ = X.shape\n         weights, means, covariances = _estimate_gaussian_parameters(X, resp, self.reg_covar, self.covariance_type)\n         weights /= n_samples\n", "normalized_occurrence": 1}
