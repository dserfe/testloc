{"model_name_or_path": "agentless_claude", "instance_id": "pytest-dev__pytest-7490", "model_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 1316da9..194178e 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -254,7 +254,15 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n-    yield\n+    yield_outcome = yield\n+\n+    # Re-evaluate xfail marks after test execution in case they were added during the test\n+    if not item.config.option.runxfail:\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+        if xfailed and yield_outcome.get_result().failed:\n+            # Test failed and was marked as xfail - should be xfailed\n+            yield_outcome.force_result(None)\n \n \n @hookimpl(hookwrapper=True)\n", "occurrence": 1, "normalized_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex d85ae4c..0040c7a 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -135,7 +135,12 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     if not item.config.option.runxfail:\n         if xfailed and (not xfailed.run):\n             xfail('[NOTRUN] ' + xfailed.reason)\n-    yield\n+    yield_outcome = (yield)\n+    if not item.config.option.runxfail:\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+        if xfailed and yield_outcome.get_result().failed:\n+            yield_outcome.force_result(None)\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = (yield)\n", "normalized_occurrence": 1}
