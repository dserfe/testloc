{"model_name_or_path": "agentless_claude", "instance_id": "pydata__xarray-6721", "model_patch": "diff --git a/xarray/core/common.py b/xarray/core/common.py\nindex 304e65c..c2810a1 100644\n--- a/xarray/core/common.py\n+++ b/xarray/core/common.py\n@@ -2023,6 +2023,20 @@ def get_chunksizes(\n \n     chunks: dict[Any, tuple[int, ...]] = {}\n     for v in variables:\n+        # First try to get chunks from encoding to avoid loading data\n+        if v.encoding and \"chunks\" in v.encoding:\n+            encoding_chunks = v.encoding[\"chunks\"]\n+            if encoding_chunks is not None:\n+                for dim, c in zip(v.dims, encoding_chunks):\n+                    if dim in chunks and c != chunks[dim]:\n+                        raise ValueError(\n+                            f\"Object has inconsistent chunks along dimension {dim}. \"\n+                            \"This can be fixed by calling unify_chunks().\"\n+                        )\n+                    chunks[dim] = c\n+                continue\n+                \n+        # Fall back to checking data chunks if not in encoding\n         if hasattr(v.data, \"chunks\"):\n             for dim, c in v.chunksizes.items():\n                 if dim in chunks and c != chunks[dim]:\n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/common.py b/xarray/core/common.py\nindex 66e6a33..af807f2 100644\n--- a/xarray/core/common.py\n+++ b/xarray/core/common.py\n@@ -387,6 +387,14 @@ def ones_like(other: Dataset | DataArray | Variable, dtype: DTypeMaybeMapping=No\n def get_chunksizes(variables: Iterable[Variable]) -> Mapping[Any, tuple[int, ...]]:\n     chunks: dict[Any, tuple[int, ...]] = {}\n     for v in variables:\n+        if v.encoding and 'chunks' in v.encoding:\n+            encoding_chunks = v.encoding['chunks']\n+            if encoding_chunks is not None:\n+                for dim, c in zip(v.dims, encoding_chunks):\n+                    if dim in chunks and c != chunks[dim]:\n+                        raise ValueError(f'Object has inconsistent chunks along dimension {dim}. This can be fixed by calling unify_chunks().')\n+                    chunks[dim] = c\n+                continue\n         if hasattr(v.data, 'chunks'):\n             for dim, c in v.chunksizes.items():\n                 if dim in chunks and c != chunks[dim]:\n", "normalized_occurrence": 1}
