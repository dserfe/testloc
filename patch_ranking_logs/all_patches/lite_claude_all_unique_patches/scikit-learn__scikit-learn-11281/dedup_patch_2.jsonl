{"model_name_or_path": "agentless_claude", "instance_id": "scikit-learn__scikit-learn-11281", "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 721709f..1f9b69c 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -96,6 +96,8 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n             raise ValueError(\"Invalid value for 'n_components': %d \"\n                              \"Estimation requires at least one component\"\n                              % self.n_components)\n+        # For clustering interface compatibility\n+        self.n_clusters = self.n_components\n \n         if self.tol < 0.:\n             raise ValueError(\"Invalid value for 'tol': %.5f \"\n@@ -188,6 +190,31 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n         -------\n         self\n         \"\"\"\n+        result = self._fit(X)\n+        # Store labels for training data\n+        self.labels_ = self.predict(X)\n+        return result\n+\n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model and predict labels for X.\n+\n+        This is more efficient than calling fit(X) followed by predict(X).\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        self.fit(X)\n+        return self.labels_\n+\n+    def _fit(self, X):\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n         self._check_initial_parameters(X)\n \n@@ -500,4 +527,4 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n             print(\"Initialization converged: %s\" % self.converged_)\n         elif self.verbose >= 2:\n             print(\"Initialization converged: %s\\t time lapse %.5fs\\t ll %.5f\" %\n-                  (self.converged_, time() - self._init_prev_time, ll))\n+                  (self.converged_, time() - self._init_prev_time, ll))\n", "occurrence": 1, "normalized_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 71fc3c3..7c3c746 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -36,6 +36,7 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     def _check_initial_parameters(self, X):\n         if self.n_components < 1:\n             raise ValueError(\"Invalid value for 'n_components': %d Estimation requires at least one component\" % self.n_components)\n+        self.n_clusters = self.n_components\n         if self.tol < 0.0:\n             raise ValueError(\"Invalid value for 'tol': %.5f Tolerance used by the EM must be non-negative\" % self.tol)\n         if self.n_init < 1:\n@@ -64,6 +65,13 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     def _initialize(self, X, resp):\n         pass\n     def fit(self, X, y=None):\n+        result = self._fit(X)\n+        self.labels_ = self.predict(X)\n+        return result\n+    def fit_predict(self, X, y=None):\n+        self.fit(X)\n+        return self.labels_\n+    def _fit(self, X):\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n         self._check_initial_parameters(X)\n         do_init = not (self.warm_start and hasattr(self, 'converged_'))\n", "normalized_occurrence": 1}
