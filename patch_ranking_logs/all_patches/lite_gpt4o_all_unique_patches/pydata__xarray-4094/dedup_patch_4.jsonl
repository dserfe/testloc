{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-4094", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 1e17b93..351338d 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3384,7 +3384,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         ----------\n         new_dim : Hashable\n             Name of the new stacked coordinate\n-        sample_dims : Sequence[Hashable]\n+        sample_dims : Sequence[Hashable],\n+        include_single_dim_vars : bool = False\n             Dimensions that **will not** be stacked. Each array in the dataset\n             must share these dimensions. For machine learning applications,\n             these define the dimensions over which samples are drawn.\n@@ -3440,7 +3441,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         Dimensions without coordinates: x\n \n         \"\"\"\n-        stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n+        if include_single_dim_vars:\n+            stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims and dim in self.dims and len(self[dim].dims) > 1)\n+        else:\n+            stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n \n         for variable in self:\n             dims = self[variable].dims\n@@ -3458,7 +3462,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                     assign_coords[dim] = None\n \n             expand_dims = set(stacking_dims).difference(set(val.dims))\n-            expand_dims.add(variable_dim)\n+            if len(val.dims) > 1:\n+                expand_dims.add(variable_dim)\n             # must be list for .expand_dims\n             expand_dims = list(expand_dims)\n \n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 21ef2a4..0ba8472 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1139,7 +1139,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             result = result._stack_once(dims, new_dim)\n         return result\n     def to_stacked_array(self, new_dim: Hashable, sample_dims: Sequence[Hashable], variable_dim: str='variable', name: Hashable=None) -> 'DataArray':\n-        stacking_dims = tuple((dim for dim in self.dims if dim not in sample_dims))\n+        if include_single_dim_vars:\n+            stacking_dims = tuple((dim for dim in self.dims if dim not in sample_dims and dim in self.dims and (len(self[dim].dims) > 1)))\n+        else:\n+            stacking_dims = tuple((dim for dim in self.dims if dim not in sample_dims))\n         for variable in self:\n             dims = self[variable].dims\n             dims_include_sample_dims = set(sample_dims) <= set(dims)\n@@ -1151,7 +1154,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 if dim not in val.dims:\n                     assign_coords[dim] = None\n             expand_dims = set(stacking_dims).difference(set(val.dims))\n-            expand_dims.add(variable_dim)\n+            if len(val.dims) > 1:\n+                expand_dims.add(variable_dim)\n             expand_dims = list(expand_dims)\n             return val.assign_coords(**assign_coords).expand_dims(expand_dims).stack({new_dim: (variable_dim,) + stacking_dims})\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n", "normalized_occurrence": 1}
