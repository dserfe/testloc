{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-4094", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 1e17b93..4914a2e 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3455,7 +3455,9 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             assign_coords = {variable_dim: val.name}\n             for dim in stacking_dims:\n                 if dim not in val.dims:\n-                    assign_coords[dim] = None\n+                    # assign None for missing dimension only if the variable has more than one dimension\n+                    if len(val.dims) > 1:\n+                        assign_coords[dim] = None\n \n             expand_dims = set(stacking_dims).difference(set(val.dims))\n             expand_dims.add(variable_dim)\n@@ -3472,15 +3474,16 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n         data_array = xr.concat(stackable_vars, dim=new_dim)\n \n-        # coerce the levels of the MultiIndex to have the same type as the\n-        # input dimensions. This code is messy, so it might be better to just\n-        # input a dummy value for the singleton dimension.\n-        idx = data_array.indexes[new_dim]\n-        levels = [idx.levels[0]] + [\n-            level.astype(self[level.name].dtype) for level in idx.levels[1:]\n-        ]\n-        new_idx = idx.set_levels(levels)\n-        data_array[new_dim] = IndexVariable(new_dim, new_idx)\n+        if stacking_dims:\n+            # coerce the levels of the MultiIndex to have the same type as the\n+            # input dimensions. This code is messy, so it might be better to just\n+            # input a dummy value for the singleton dimension.\n+            idx = data_array.indexes[new_dim]\n+            levels = [idx.levels[0]] + [\n+                level.astype(self[level.name].dtype) for level in idx.levels[1:]\n+            ]\n+            new_idx = idx.set_levels(levels)\n+            data_array[new_dim] = IndexVariable(new_dim, new_idx)\n \n         if name is not None:\n             data_array.name = name\n", "occurrence": 1, "normalized_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 21ef2a4..ec7ee8b 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1149,17 +1149,19 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             assign_coords = {variable_dim: val.name}\n             for dim in stacking_dims:\n                 if dim not in val.dims:\n-                    assign_coords[dim] = None\n+                    if len(val.dims) > 1:\n+                        assign_coords[dim] = None\n             expand_dims = set(stacking_dims).difference(set(val.dims))\n             expand_dims.add(variable_dim)\n             expand_dims = list(expand_dims)\n             return val.assign_coords(**assign_coords).expand_dims(expand_dims).stack({new_dim: (variable_dim,) + stacking_dims})\n         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n         data_array = xr.concat(stackable_vars, dim=new_dim)\n-        idx = data_array.indexes[new_dim]\n-        levels = [idx.levels[0]] + [level.astype(self[level.name].dtype) for level in idx.levels[1:]]\n-        new_idx = idx.set_levels(levels)\n-        data_array[new_dim] = IndexVariable(new_dim, new_idx)\n+        if stacking_dims:\n+            idx = data_array.indexes[new_dim]\n+            levels = [idx.levels[0]] + [level.astype(self[level.name].dtype) for level in idx.levels[1:]]\n+            new_idx = idx.set_levels(levels)\n+            data_array[new_dim] = IndexVariable(new_dim, new_idx)\n         if name is not None:\n             data_array.name = name\n         return data_array\n", "normalized_occurrence": 1}
